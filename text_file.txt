Here's a README file tailored for your AutoBI project, taking inspiration from the structure of the VSwarm README:
 
 
---
 
<div align="center"><img alt="AutoBI" height="200px" src="./img/logo.png"><h1>AutoBI</h1><p><b>Low code tool to convert text into SQL queries and interact with various databases using LLMs</b></p></div>Table of Contents
 
What is AutoBI?
 
Use Cases
 
Highlights
 
How to Get Started
 
Generate Secret Keys
 
Deploy AutoBI Locally with Docker
 
Deploy AutoBI on a Remote Server
 
 
Guides and Concepts
 
Sequential vs Hierarchical Workflows
 
Creating a Skill using Skill Definitions
 
Writing a Custom Skill using LangChain
 
Retrieval Augmented Generation (RAG)
 
Customizing Embedding Models
 
 
Contribution
 
License
 
 
What is AutoBI?
 
AutoBI is a robust tool that leverages Large Language Models (LLMs) to transform natural language into SQL queries, allowing users to query data from multiple databases with ease. Using LangChain, AutoBI provides a seamless way to interact with PostgreSQL, Redis, and other database systems through an intuitive interface, empowering businesses to extract actionable insights without needing extensive SQL knowledge.
 
Use Cases
 
AutoBI can be applied in a variety of domains, including but not limited to:
 
Business Intelligence: AutoBI enables business users to generate insights from various databases by converting natural language questions into SQL queries.
 
Data Exploration: Analysts can explore datasets in different databases without having to write complex SQL queries.
 
Customer Support: AutoBI can help support teams access data by asking natural language queries, making customer service more efficient.
 
Ad-hoc Reporting: AutoBI allows quick generation of reports by querying multiple tables across databases.
 
 
Highlights
 
Persistent conversations: AutoBI saves and maintains conversation histories, enabling continuous and evolving data analysis.
 
Observability: Monitor and track query performance and outputs in real-time, ensuring efficient data retrieval.
 
Tool Calling: AutoBI integrates external tools and APIs, expanding its capabilities.
 
Retrieval Augmented Generation: AutoBI augments your LLM's reasoning with your internal knowledge base.
 
Human-In-The-Loop: For critical tasks, AutoBI enables human approval before query execution.
 
Open Source Models: Use open-source LLM models like GPT, Gemini, and others.
 
Easy Deployment: Deploy AutoBI with Docker for seamless and scalable operations.
 
Multi-Tenancy: Manage and support multiple users and teams with ease.
 
 
How to Get Started
 
Generate Secret Keys
 
You need to generate secret keys to secure your AutoBI instance. Run the following command to generate a secret key:
 
python -c "import secrets; print(secrets.token_urlsafe(32))"
 
Copy and use the generated key for SECRET_KEY, POSTGRES_PASSWORD, etc. You can run the command multiple times to generate additional keys.
 
Deploy AutoBI Locally with Docker
 
To deploy AutoBI on your local machine using Docker, follow the local deployment guide.
 
Deploy AutoBI on a Remote Server
 
For deploying AutoBI on a remote server, refer to the remote deployment guide.
 
Guides and Concepts
 
Sequential vs Hierarchical Workflows
 
AutoBI supports both sequential and hierarchical workflows for query execution. In a sequential workflow, queries are executed one after another. In a hierarchical workflow, complex queries are broken down and delegated to different agents or sub-teams for execution.
 
Creating a Skill Using Skill Definitions
 
You can create custom skills that allow agents to interact with external APIs, fetch data, or execute queries. Simply provide a skill definition in the 'Skills' tab and configure the parameters.
 
Writing a Custom Skill using LangChain
 
For more advanced capabilities, you can write custom skills using LangChain. Refer to the LangChain documentation for guidance.
 
Retrieval Augmented Generation (RAG)
 
RAG enables AutoBI to reason with additional private data uploaded into its knowledge base. You can select specific datasets for agents to reason with, making their responses more specialized.
 
Customizing Embedding Models
 
By default, AutoBI uses BAAI/bge-small-en-v1.5. You can customize the embedding model by editing the DENSE_EMBEDDING_MODEL in the environment configuration. Refer to the list of supported models.
 
Contribution
 
AutoBI is an open-source project, and we welcome contributions! To get started, check out our contribution guide.
 
Some ways to contribute:
 
Report bugs and issues
 
Enhance our documentation
 
Suggest or contribute new features
 
 
License
 
AutoBI is licensed under the MIT license. Check the license file for more details.
 
 
---
 
This README provides a clear and structured overview of the AutoBI project, similar to the VSwarm README, but tailored to your specific project. Let me know if you need further adjustments!







organisation backend 
app.py->

from fastapi import FastAPI, HTTPException, Depends
from sqlalchemy.orm import Session
from models import Organization, OrganizationCreate
from db import get_db, Base, engine

app = FastAPI()

# Initialize the database tables
Base.metadata.create_all(bind=engine)

@app.get("/organizations")
def get_organizations(db: Session = Depends(get_db)):
    organizations = db.query(Organization).all()
    return organizations

@app.post("/organizations", status_code=201)
def add_organization(organization: OrganizationCreate, db: Session = Depends(get_db)):
    new_org = Organization(name=organization.name, description=organization.description)
    db.add(new_org)
    db.commit()
    db.refresh(new_org)
    return new_org

@app.delete("/organizations/{org_id}", status_code=204)
def delete_organization(org_id: int, db: Session = Depends(get_db)):
    org = db.query(Organization).filter(Organization.id == org_id).first()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    db.delete(org)
    db.commit()
    return {"detail": "Organization deleted successfully"}



models.py->
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from pydantic import BaseModel

Base = declarative_base()

class Organization(Base):
    __tablename__ = "organizations"
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(100), nullable=False)
    description = Column(String(500), nullable=False)

class OrganizationCreate(BaseModel):
    name: str
    description: str



db.py->
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

SQLALCHEMY_DATABASE_URL = "sqlite:///./organizations.db"

engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

# Dependency for FastAPI routes
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()




db_init->

from db import engine, Base

Base.metadata.create_all(bind=engine)
print("Database initialized successfully.")


requirements.txt->
fastapi==0.95.2
uvicorn==0.23.1
SQLAlchemy==1.4.49
pydantic==1.10.7



























app.py

from fastapi import FastAPI, HTTPException, Depends
from sqlalchemy.orm import Session
from models import Organization, OrganizationCreate, User
from db import get_db, Base, engine


app = FastAPI()

# Ensure the database tables are created
print("Creating tables...")
Base.metadata.create_all(bind=engine)
print("Tables created successfully!")


@app.get("/organizations")
def get_organizations(db: Session = Depends(get_db)):
    organizations = db.query(Organization).all()
    return organizations

@app.post("/organizations", status_code=201)
def add_organization(organization: OrganizationCreate, db: Session = Depends(get_db)):
    new_org = Organization(name=organization.name, description=organization.description)
    db.add(new_org)
    db.commit()
    db.refresh(new_org)
    return new_org

@app.delete("/organizations/{org_id}", status_code=204)
def delete_organization(org_id: int, db: Session = Depends(get_db)):
    org = db.query(Organization).filter(Organization.id == org_id).first()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    db.delete(org)
    db.commit()
    return {"detail": "Organization deleted successfully"}

@app.put("/organizations/{org_id}", status_code=200)
def update_organization(org_id: int, organization: OrganizationCreate, db: Session = Depends(get_db)):
    org = db.query(Organization).filter(Organization.id == org_id).first()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    org.name = organization.name
    org.description = organization.description
    db.commit()
    return org

























# from fastapi import FastAPI
# from sqlalchemy import create_engine
# from sqlalchemy.ext.declarative import declarative_base
# from sqlalchemy.orm import sessionmaker
# from models import Base  # Assuming your models are defined in models.py

# SQLALCHEMY_DATABASE_URL = "sqlite:///./organizations.db"

# engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
# SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# # Create the tables
# Base.metadata.create_all(bind=engine)

# app = FastAPI()

# # Dependency
# def get_db():
#     db = SessionLocal()
#     try:
#         yield db
#     finally:
#         db.close()





models.py

from sqlalchemy import Column, Integer, String, Boolean
from sqlalchemy.ext.declarative import declarative_base
from pydantic import BaseModel

Base = declarative_base()

class Organization(Base):
    __tablename__ = "table_org"
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(100), nullable=False)
    description = Column(String(500), nullable=False)

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String(50), unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    is_superadmin = Column(Boolean, default=False)

class OrganizationCreate(BaseModel):
    name: str
    description: str



db.py


import os
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

from models import Base  # Assuming your models are defined in models.py

SQLALCHEMY_DATABASE_URL = "sqlite:///./organizations.db"
print(f"Database path: {os.path.abspath('organizations.db')}")

engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

# Dependency for FastAPI routes
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()









C:\Users\trish1u\Desktop\my>uvicorn app:app --reload
INFO:     Will watch for changes in these directories: ['C:\\Users\\trish1u\\Desktop\\my']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [7200] using StatReload
ERROR:    Error loading ASGI app. Could not import module "app".





C:\Users\trish1u\Desktop\my>sqlite3 organizations.db
'sqlite3' is not recognized as an internal or external command,
operable program or batch file.




import os
from sqlmodel import SQLModel, Session, create_engine

from models import Organization  # Assuming your models are defined in models.py

# Database URL
SQLALCHEMY_DATABASE_URL = "sqlite:///./organizations.db"
print(f"Database path: {os.path.abspath('organizations.db')}")

# Create engine
engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})

# Create the database tables if they don't exist
SQLModel.metadata.create_all(engine)

# Dependency for FastAPI routes
def get_db():
    with Session(engine) as session:
        yield session




app.py

from fastapi import FastAPI, HTTPException, Depends
from sqlmodel import Session, select
from models import Organization, OrganizationCreate
from db import get_db, engine

app = FastAPI()

# Ensure the database tables are created
print("Creating tables...")
SQLModel.metadata.create_all(bind=engine)
print("Tables created successfully!")


@app.get("/organizations")
def get_organizations(db: Session = Depends(get_db)):
    statement = select(Organization)
    results = db.exec(statement)
    organizations = results.all()
    return organizations

@app.post("/organizations", status_code=201)
def add_organization(organization: OrganizationCreate, db: Session = Depends(get_db)):
    new_org = Organization(name=organization.name, description=organization.description)
    db.add(new_org)
    db.commit()
    db.refresh(new_org)
    return new_org

@app.delete("/organizations/{org_id}", status_code=204)
def delete_organization(org_id: int, db: Session = Depends(get_db)):
    statement = select(Organization).where(Organization.id == org_id)
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    db.delete(org)
    db.commit()
    return {"detail": "Organization deleted successfully"}

@app.put("/organizations/{org_id}", status_code=200)
def update_organization(org_id: int, organization: OrganizationCreate, db: Session = Depends(get_db)):
    statement = select(Organization).where(Organization.id == org_id)
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    org.name = organization.name
    org.description = organization.description
    db.commit()
    return org











models.py

from sqlmodel import SQLModel, Field
from typing import Optional

# Define the Organization model
class Organization(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True, index=True)
    name: str = Field(max_length=100, nullable=False)
    description: str = Field(max_length=500, nullable=False)

# Define the User model
class User(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True, index=True)
    username: str = Field(max_length=50, unique=True, index=True, nullable=False)
    hashed_password: str = Field(nullable=False)
    is_superadmin: bool = Field(default=False)

# Pydantic model for creating an organization
class OrganizationCreate(SQLModel):
    name: str
    description: str





from fastapi import FastAPI, HTTPException, Depends
from sqlmodel import Session, select, SQLModel
from models import Organization, OrganizationCreate
from db import get_db, engine

app = FastAPI()

# Ensure the database tables are created
print("Creating tables...")
SQLModel.metadata.create_all(bind=engine)
print("Tables created successfully!")


@app.get("/organizations")
def get_organizations(db: Session = Depends(get_db)):
    statement = select(Organization)
    results = db.exec(statement)
    organizations = results.all()
    return organizations

@app.post("/organizations", status_code=201)
def add_organization(organization: OrganizationCreate, db: Session = Depends(get_db)):
    new_org = Organization(name=organization.name, description=organization.description)
    db.add(new_org)
    db.commit()
    db.refresh(new_org)
    return new_org

@app.delete("/organizations/{org_id}", status_code=204)
def delete_organization(org_id: int, db: Session = Depends(get_db)):
    statement = select(Organization).where(Organization.id == org_id)
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    db.delete(org)
    db.commit()
    return {"detail": "Organization deleted successfully"}

@app.put("/organizations/{org_id}", status_code=200)
def update_organization(org_id: int, organization: OrganizationCreate, db: Session = Depends(get_db)):
    statement = select(Organization).where(Organization.id == org_id)
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    org.name = organization.name
    org.description = organization.description
    db.commit()
    return org


models.py
from datetime import datetime, timezone
from typing import Optional
from uuid import UUID, uuid4

from sqlalchemy import JSON, Column
from sqlmodel import Field, Relationship, SQLModel

from sqlmodel import SQLModel, Field
from typing import Optional



class Token(SQLModel):
    access_token: str
    token_type: str = "bearer"


# Association Table for User and Workspace with role and domains
class UserWorkspaceLink(SQLModel, table=True):
    __tablename__ = "user_workspace_link"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    role: str  # Role can be 'workspace_admin', 'domain_admin', or 'api_user'
    domains: list[UUID] = Field(
        sa_column=Column(JSON)
    )  # Placeholder for future domain relationship


# Association Table for favorited workspaces
class UserWorkspaceFavorited(SQLModel, table=True):
    __tablename__ = "user_workspace_favorited"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    favorited_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


# User Base Model
class UserBase(SQLModel):
    full_name: str
    username: str
    email: str
    functional_role: str  # Role like 'super_user' or 'application_user'
    is_active: bool = True


# User Table Model
class User(UserBase, table=True):
    __tablename__ = "user"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    hashed_password: str
    created: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated: datetime = datetime.now(timezone.utc)
    is_first_login: bool = True

    # Relationships
    workspaces: list["Workspace"] = Relationship(
        back_populates="users", link_model=UserWorkspaceLink
    )
    # Favorited workspaces
    favorited_workspaces: list["Workspace"] = Relationship(
        back_populates="favorited_by", link_model=UserWorkspaceFavorited
    )


# Pydantic Models for login
class UserLogin(SQLModel):
    username: str
    password: str
    domain: str | None = None
    method: str


# Pydantic Models for User
class UserCreate(UserBase):
    password: str


class UserRead(UserBase):
    id: UUID
    created: datetime
    last_updated: datetime 
    is_first_login: bool


# Workspace Base Model
class WorkspaceBase(SQLModel):
    description: str 
    name: str
    default_llm_provider: str
    default_embedding_provider: str 
    default_embedding_model: str 
    default_llm_model: str 
    is_public: bool = False


# Workspace Table Model
class Workspace(WorkspaceBase, table=True):
    __tablename__ = "workspace"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    organization_id: UUID = Field(foreign_key="admin_org_settings.id")
    created_by_id: UUID = Field(foreign_key="user.id")
    tags: list[str] = Field(sa_column=Column(JSON))  # Redefine 'tags' with sa_column

    created_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated_date: datetime 

    # Relationships
    users: list["User"] = Relationship(
        back_populates="workspaces", link_model=UserWorkspaceLink
    )
    favorited_by: list["User"] = Relationship(
        back_populates="favorited_workspaces", link_model=UserWorkspaceFavorited
    )


# Pydantic Models for Workspace
class WorkspaceCreate(WorkspaceBase):
    tags: list[str] | None = None  # Include 'tags' here for input
    organization_id: UUID 


class WorkspaceRead(WorkspaceBase):
    id: UUID
    tags: list[str] | None = None  # Include 'tags' here for output
    created_by_id: UUID 
    created_date: datetime
    last_updated_date: datetime 


# AdminOrgSettings Base Model
class AdminOrgSettingsBase(SQLModel):
    name: str
    description: str 


# AdminOrgSettings Table Model
class AdminOrgSettings(AdminOrgSettingsBase, table=True):
    __tablename__ = "admin_org_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminOrgSettings
class AdminOrgSettingsCreate(AdminOrgSettingsBase):
    pass


class AdminOrgSettingsRead(AdminOrgSettingsBase):
    id: UUID


# AdminLDAPSettings Base Model
class AdminLDAPSettingsBase(SQLModel):
    ldap_name: str
    ldap_server: str
    ldap_port: int
    ldap_base_dn: str 
    ldap_base_search: str 
    ldap_ssl_enabled: bool = False


# AdminLDAPSettings Table Model
class AdminLDAPSettings(AdminLDAPSettingsBase, table=True):
    __tablename__ = "admin_ldap_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLDAPSettings
class AdminLDAPSettingsCreate(AdminLDAPSettingsBase):
    pass


class AdminLDAPSettingsRead(AdminLDAPSettingsBase):
    id: UUID


# AdminLLMModelSettings Base Model
class AdminLLMModelSettingsBase(SQLModel):
    llm_model_name: str
    llm_provider_id: UUID
    llm_base_endpoint_url: str | None = None
    llm_api_token: str


# AdminLLMModelSettings Table Model
class AdminLLMModelSettings(AdminLLMModelSettingsBase, table=True):
    __tablename__ = "admin_llm_model_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLLMModelSettings
class AdminLLMModelSettingsCreate(AdminLLMModelSettingsBase):
    pass


class AdminLLMModelSettingsRead(AdminLLMModelSettingsBase):
    id: UUID


# AdminLLMProviderSettings Base Model
class AdminLLMProviderSettingsBase(SQLModel):
    llm_provider_name: str


# AdminLLMProviderSettings Table Model
class AdminLLMProviderSettings(AdminLLMProviderSettingsBase, table=True):
    __tablename__ = "admin_llm_provider_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLLMProviderSettings
class AdminLLMProviderSettingsCreate(AdminLLMProviderSettingsBase):
    pass


class AdminLLMProviderSettingsRead(AdminLLMProviderSettingsBase):
    id: UUID


# Define the Organization model
class Organization(SQLModel, table=True):
    id: UUID = Field(default=None, primary_key=True, index=True)
    # Optional[int] = Field(default=None, primary_key=True, index=True)
    name: str = Field(max_length=100, nullable=False)
    description: str = Field(max_length=500, nullable=False)

# # Define the User model
# class User(SQLModel, table=True):
#     id: Optional[int] = Field(default=None, primary_key=True, index=True)
#     username: str = Field(max_length=50, unique=True, index=True, nullable=False)
#     hashed_password: str = Field(nullable=False)
    # is_superadmin: bool = Field(default=False)

# Pydantic model for creating an organization
class OrganizationCreate(SQLModel):
    name: str
    description: str







organization.py
from fastapi import FastAPI, HTTPException, Depends, APIRouter
from sqlmodel import Session, select, SQLModel
from app.models import *
from app.api.deps import get_db


router = APIRouter()

# Ensure the database tables are created
# print("Creating tables...")
# SQLModel.metadata.create_all(bind=engine)
# print("Tables created successfully!")


@router.get("/organizations")
def get_organizations(db: Session = Depends(get_db)):
    statement = select(AdminOrgSettings)
    results = db.exec(statement)
    organizations = results.all()
    return organizations

@router.post("/organizations", status_code=201)
def add_organization(organization: OrganizationCreate, db: Session = Depends(get_db)):
    # new_org = admin_org_settings()
    # statement = select(admin_org_settings).where(admin_org_settings.id==org_id).first()
    new_org = AdminOrgSettings(name=organization.name, description=organization.description)
    db.add(new_org)
    db.commit()
    db.refresh(new_org)
    return new_org

@router.delete("/organizations/{org_id}", status_code=204)
def delete_organization(org_id: str, db: Session = Depends(get_db)):
    statement = select(AdminOrgSettings).where(AdminOrgSettings.id == UUID(org_id))
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    db.delete(org)
    db.commit()
    return {"detail": "Organization deleted successfully"}

@router.put("/organizations/{org_id}", status_code=200)
def update_organization(org_id: str, organization: OrganizationCreate, db: Session = Depends(get_db)):
    statement = select(AdminOrgSettings).where(AdminOrgSettings.id == UUID(org_id))
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    org.name = organization.name
    org.description = organization.description
    db.commit()
    return org




main.py

from fastapi import APIRouter, FastAPI
# from routes import organization
import sys

from app.api.routes import add_user, login, organization

api_router = APIRouter()
api_router.include_router(login.router, tags=["login"])
api_router.include_router(add_user.router, tags=["users"])



api_router.include_router(organization.router, tags=["organization"])


models.py

from datetime import datetime, timezone
from typing import Optional
from uuid import UUID, uuid4

from sqlalchemy import JSON, Column
from sqlmodel import Field, Relationship, SQLModel

from sqlmodel import SQLModel, Field
from typing import Optional



class Token(SQLModel):
    access_token: str
    token_type: str = "bearer"


# Association Table for User and Workspace with role and domains
class UserWorkspaceLink(SQLModel, table=True):
    __tablename__ = "user_workspace_link"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    role: str  # Role can be 'workspace_admin', 'domain_admin', or 'api_user'
    domains: list[UUID] = Field(
        sa_column=Column(JSON)
    )  # Placeholder for future domain relationship


# Association Table for favorited workspaces
class UserWorkspaceFavorited(SQLModel, table=True):
    __tablename__ = "user_workspace_favorited"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    favorited_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


# User Base Model
class UserBase(SQLModel):
    full_name: str
    username: str
    email: str
    functional_role: str  # Role like 'super_user' or 'application_user'
    is_active: bool = True


# User Table Model
class User(UserBase, table=True):
    __tablename__ = "user"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    hashed_password: str
    created: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated: datetime = datetime.now(timezone.utc)
    is_first_login: bool = True

    # Relationships
    workspaces: list["Workspace"] = Relationship(
        back_populates="users", link_model=UserWorkspaceLink
    )
    # Favorited workspaces
    favorited_workspaces: list["Workspace"] = Relationship(
        back_populates="favorited_by", link_model=UserWorkspaceFavorited
    )


# Pydantic Models for login
class UserLogin(SQLModel):
    username: str
    password: str
    domain: str | None = None
    method: str


# Pydantic Models for User
class UserCreate(UserBase):
    password: str


class UserRead(UserBase):
    id: UUID
    created: datetime
    last_updated: datetime 
    is_first_login: bool


# Workspace Base Model
class WorkspaceBase(SQLModel):
    description: str 
    name: str
    default_llm_provider: str
    default_embedding_provider: str 
    default_embedding_model: str 
    default_llm_model: str 
    is_public: bool = False


# Workspace Table Model
class Workspace(WorkspaceBase, table=True):
    __tablename__ = "workspace"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    organization_id: UUID = Field(foreign_key="admin_org_settings.id")
    created_by_id: UUID = Field(foreign_key="user.id")
    tags: list[str] = Field(sa_column=Column(JSON))  # Redefine 'tags' with sa_column

    created_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated_date: datetime 

    # Relationships
    users: list["User"] = Relationship(
        back_populates="workspaces", link_model=UserWorkspaceLink
    )
    favorited_by: list["User"] = Relationship(
        back_populates="favorited_workspaces", link_model=UserWorkspaceFavorited
    )


# Pydantic Models for Workspace
class WorkspaceCreate(WorkspaceBase):
    tags: list[str] | None = None  # Include 'tags' here for input
    organization_id: UUID 


class WorkspaceRead(WorkspaceBase):
    id: UUID
    tags: list[str] | None = None  # Include 'tags' here for output
    created_by_id: UUID 
    created_date: datetime
    last_updated_date: datetime 


# AdminOrgSettings Base Model
class AdminOrgSettingsBase(SQLModel):
    name: str
    description: str 


# AdminOrgSettings Table Model
class AdminOrgSettings(AdminOrgSettingsBase, table=True):
    __tablename__ = "admin_org_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminOrgSettings
class AdminOrgSettingsCreate(AdminOrgSettingsBase):
    pass


class AdminOrgSettingsRead(AdminOrgSettingsBase):
    id: UUID


# AdminLDAPSettings Base Model
class AdminLDAPSettingsBase(SQLModel):
    ldap_name: str
    ldap_server: str
    ldap_port: int
    ldap_base_dn: str 
    ldap_base_search: str 
    ldap_ssl_enabled: bool = False


# AdminLDAPSettings Table Model
class AdminLDAPSettings(AdminLDAPSettingsBase, table=True):
    __tablename__ = "admin_ldap_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLDAPSettings
class AdminLDAPSettingsCreate(AdminLDAPSettingsBase):
    pass


class AdminLDAPSettingsRead(AdminLDAPSettingsBase):
    id: UUID


# AdminLLMModelSettings Base Model
class AdminLLMModelSettingsBase(SQLModel):
    llm_model_name: str
    llm_provider_id: UUID
    llm_base_endpoint_url: str | None = None
    llm_api_token: str


# AdminLLMModelSettings Table Model
class AdminLLMModelSettings(AdminLLMModelSettingsBase, table=True):
    __tablename__ = "admin_llm_model_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLLMModelSettings
class AdminLLMModelSettingsCreate(AdminLLMModelSettingsBase):
    pass


class AdminLLMModelSettingsRead(AdminLLMModelSettingsBase):
    id: UUID


# AdminLLMProviderSettings Base Model
class AdminLLMProviderSettingsBase(SQLModel):
    llm_provider_name: str


# AdminLLMProviderSettings Table Model
class AdminLLMProviderSettings(AdminLLMProviderSettingsBase, table=True):
    __tablename__ = "admin_llm_provider_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLLMProviderSettings
class AdminLLMProviderSettingsCreate(AdminLLMProviderSettingsBase):
    pass


class AdminLLMProviderSettingsRead(AdminLLMProviderSettingsBase):
    id: UUID


# Define the Organization model
# class Organization(SQLModel, table=True):
#     id: UUID = Field(default_factory=uuid4, primary_key=True)
#     # Optional[int] = Field(default=None, primary_key=True, index=True)
#     name: str = Field(max_length=100, nullable=False)
#     description: str = Field(max_length=500, nullable=False)

# # Define the User model
# class User(SQLModel, table=True):
#     id: Optional[int] = Field(default=None, primary_key=True, index=True)
#     username: str = Field(max_length=50, unique=True, index=True, nullable=False)
#     hashed_password: str = Field(nullable=False)
    # is_superadmin: bool = Field(default=False)

# Pydantic model for creating an organization
class OrganizationCreate(SQLModel):
    name: str = Field(max_length=50, unique=True, index=True, nullable=False)
    description: str






















Srili

from app.api.deps import get_db
from fastapi import APIRouter, HTTPException, Depends,Query
from app.models import *
from typing import List,Optional
from sqlmodel import Session,select
from sqlalchemy import or_,func
from uuid import UUID, uuid4
from rapidfuzz import fuzz


router = APIRouter()

@router.get("/workspace/search",response_model = List[Workspace])
def search_anything(search_text:Optional[str] = None,
                    visibility:Optional[str] = None,
                    tags:Optional[List[str]] = Query(None),
                    Organization:Optional[str]=None,
                    session: Session = Depends(get_db),
                    skip:int = 0,
                    limit:int=10)-> List[Workspace]:
    
    query = select(Workspace).where(Workspace.is_active == True)
    
    if visibility:
        if visibility.lower() == "public":
            query = query.where(Workspace.is_public == True)
        else:
            query = query.where(Workspace.is_public == False)
    if tags:
        tag_conditions = [func.instr(Workspace.tags,tag) > 0 for tag in tags]
        query = query.where(or_(*tag_conditions))
            
    if Organization:
        
        org_settings = session.exec(select(AdminOrgSettings).where(AdminOrgSettings.name == Organization))
        org_id = org_settings.id
        if isinstance(org_id,str):
            try:
                org_id = UUID(org_id)
            except ValueError as e:
                print(e)
            
        query = query.where(Workspace.organization_id == org_id)
    
    filtered_workspaces = session.exec(query).all()
    
    if search_text:
        search_text = search_text.lower()
        fuzzy_matches = []
        for workspace in filtered_workspaces:
            name_similarity = fuzz.ratio(search_text,workspace.name.lower())
            description_similarity = fuzz.ratio(search_text,workspace.description.lower())
            if name_similarity >= 80 or description_similarity >= 80:
                fuzzy_matches.append(workspace)
        
        return fuzzy_matches
        
    return filtered_workspaces










organization.py

from fastapi import FastAPI, HTTPException, Depends, APIRouter
from fastapi.responses import JSONResponse
from sqlmodel import Session, select, SQLModel
from app.models import *
from app.api.deps import get_db

router = APIRouter()

@router.get("/organizations")
def get_organizations(db: Session = Depends(get_db)):
    statement = select(AdminOrgSettings)
    results = db.exec(statement)
    organizations = results.all()
    return organizations

@router.post("/organizations", status_code=201)
def add_organization(organization: OrganizationCreate, db: Session = Depends(get_db)):
    # new_org = admin_org_settings()
    # statement = select(admin_org_settings).where(admin_org_settings.id==org_id).first()
    # new_org = AdminOrgSettings(name=organization.name, description=organization.description)
    
    statement= select(AdminOrgSettings).where(AdminOrgSettings.name == organization.name)
    existing_org= db.exec(statement).first()
    if existing_org:
        # raise HTTPException(status_code=400, detail="Organization name already used")
        return JSONResponse(status_code=200, content={"error": "Organization name already used"})
    new_org = AdminOrgSettings(name=organization.name, description=organization.description)
    db.add(new_org)
    db.commit()
    db.refresh(new_org)
    return new_org

@router.delete("/organizations/{org_id}", status_code=204)
def delete_organization(org_id: str, db: Session = Depends(get_db)):
    statement = select(AdminOrgSettings).where(AdminOrgSettings.id == UUID(org_id))
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    db.delete(org)
    db.commit()
    return {"detail": "Organization deleted successfully"}

@router.put("/organizations/{org_id}", status_code=200)
def update_organization(org_id: str, organization: OrganizationCreate, db: Session = Depends(get_db)):
    statement = select(AdminOrgSettings).where(AdminOrgSettings.id == UUID(org_id))
    results = db.exec(statement)
    org = results.one_or_none()
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    org.name = organization.name
    org.description = organization.description
    db.commit()
    return org


models.py

from datetime import datetime, timezone
from typing import Optional
from uuid import UUID, uuid4

from sqlalchemy import JSON, Column
from sqlmodel import Field, Relationship, SQLModel

from sqlmodel import SQLModel, Field
from typing import Optional



class Token(SQLModel):
    access_token: str
    token_type: str = "bearer"


# Association Table for User and Workspace with role and domains
class UserWorkspaceLink(SQLModel, table=True):
    __tablename__ = "user_workspace_link"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    role: str  # Role can be 'workspace_admin', 'domain_admin', or 'api_user'
    domains: list[UUID] = Field(
        sa_column=Column(JSON)
    )  # Placeholder for future domain relationship


# Association Table for favorited workspaces
class UserWorkspaceFavorited(SQLModel, table=True):
    __tablename__ = "user_workspace_favorited"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    favorited_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


# User Base Model
class UserBase(SQLModel):
    full_name: str
    username: str
    email: str
    functional_role: str  # Role like 'super_user' or 'application_user'
    is_active: bool = True


# User Table Model
class User(UserBase, table=True):
    __tablename__ = "user"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    hashed_password: str
    created: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated: datetime = datetime.now(timezone.utc)
    is_first_login: bool = True

    # Relationships
    workspaces: list["Workspace"] = Relationship(
        back_populates="users", link_model=UserWorkspaceLink
    )
    # Favorited workspaces
    favorited_workspaces: list["Workspace"] = Relationship(
        back_populates="favorited_by", link_model=UserWorkspaceFavorited
    )


# Pydantic Models for login
class UserLogin(SQLModel):
    username: str
    password: str
    domain: str | None = None
    method: str


# Pydantic Models for User
class UserCreate(UserBase):
    password: str


class UserRead(UserBase):
    id: UUID
    created: datetime
    last_updated: datetime 
    is_first_login: bool


# Workspace Base Model
class WorkspaceBase(SQLModel):
    description: str 
    name: str
    default_llm_provider: str
    default_embedding_provider: str 
    default_embedding_model: str 
    default_llm_model: str 
    is_public: bool = False


# Workspace Table Model
class Workspace(WorkspaceBase, table=True):
    __tablename__ = "workspace"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    organization_id: UUID = Field(foreign_key="admin_org_settings.id")
    created_by_id: UUID = Field(foreign_key="user.id")
    tags: list[str] = Field(sa_column=Column(JSON))  # Redefine 'tags' with sa_column

    created_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated_date: datetime 

    # Relationships
    users: list["User"] = Relationship(
        back_populates="workspaces", link_model=UserWorkspaceLink
    )
    favorited_by: list["User"] = Relationship(
        back_populates="favorited_workspaces", link_model=UserWorkspaceFavorited
    )

class tag(SQLModel, table= True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    object_type: str =Field(nullable= False)
    object_id: UUID = Field(default_factory=uuid4)
    tag_name: str = Field(nullable=False)
    created_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    # created_date: datetime= Field(default_factory=datetime.utcnow)

    @validator("object_type")
    def validate_object_type(cls,value):
        allowed_types = {"workspace", "domain", "document"}
        if value not in allowed_types:
            raise ValueError(f"object_type must be one of {allowed_types}")
        return value

    # id: int = Field(default=None, primary_key=True)
    # object_type: str =Field(nullable= False)
    # object_id: int = Field(nullable=False)
    # tag_name: str = Field(nullable=False)
    # created_date: datetime= Field(default_factory=datetime.utcnow)

# Pydantic Models for Workspace
class WorkspaceCreate(WorkspaceBase):
    tags: list[str] | None = None  # Include 'tags' here for input
    organization_id: UUID 


class WorkspaceRead(WorkspaceBase):
    id: UUID
    tags: list[str] | None = None  # Include 'tags' here for output
    created_by_id: UUID 
    created_date: datetime
    last_updated_date: datetime 


# AdminOrgSettings Base Model
class AdminOrgSettingsBase(SQLModel):
    name: str
    description: str 


# AdminOrgSettings Table Model
class AdminOrgSettings(AdminOrgSettingsBase, table=True):
    __tablename__ = "admin_org_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    # name: str = Field(index=True, unique=True)
    # description: str


# Pydantic Models for AdminOrgSettings
class AdminOrgSettingsCreate(AdminOrgSettingsBase):
    pass


class AdminOrgSettingsRead(AdminOrgSettingsBase):
    id: UUID


# AdminLDAPSettings Base Model
class AdminLDAPSettingsBase(SQLModel):
    ldap_name: str
    ldap_server: str
    ldap_port: int
    ldap_base_dn: str 
    ldap_base_search: str 
    ldap_ssl_enabled: bool = False


# AdminLDAPSettings Table Model
class AdminLDAPSettings(AdminLDAPSettingsBase, table=True):
    __tablename__ = "admin_ldap_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLDAPSettings
class AdminLDAPSettingsCreate(AdminLDAPSettingsBase):
    pass


class AdminLDAPSettingsRead(AdminLDAPSettingsBase):
    id: UUID


# AdminLLMModelSettings Base Model
class AdminLLMModelSettingsBase(SQLModel):
    llm_model_name: str
    llm_provider_id: UUID
    llm_base_endpoint_url: str | None = None
    llm_api_token: str


# AdminLLMModelSettings Table Model
class AdminLLMModelSettings(AdminLLMModelSettingsBase, table=True):
    __tablename__ = "admin_llm_model_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLLMModelSettings
class AdminLLMModelSettingsCreate(AdminLLMModelSettingsBase):
    pass


class AdminLLMModelSettingsRead(AdminLLMModelSettingsBase):
    id: UUID


# AdminLLMProviderSettings Base Model
class AdminLLMProviderSettingsBase(SQLModel):
    llm_provider_name: str


# AdminLLMProviderSettings Table Model
class AdminLLMProviderSettings(AdminLLMProviderSettingsBase, table=True):
    __tablename__ = "admin_llm_provider_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLLMProviderSettings
class AdminLLMProviderSettingsCreate(AdminLLMProviderSettingsBase):
    pass


class AdminLLMProviderSettingsRead(AdminLLMProviderSettingsBase):
    id: UUID


# Pydantic model for creating an organization
class OrganizationCreate(SQLModel):
    name: str
    description: str









Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Scripts\alembic.exe\__main__.py", line 7, in <module>
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\alembic\config.py", line 630, in main
    CommandLine(prog=prog).main(argv=argv)
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\alembic\config.py", line 624, in main
    self.run_cmd(cfg, options)
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\alembic\config.py", line 601, in run_cmd
    fn(
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\alembic\command.py", line 234, in revision
    script_directory.run_env()
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\alembic\script\base.py", line 579, in run_env       
    util.load_python_file(self.dir, "env.py")
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\alembic\util\pyfiles.py", line 93, in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\alembic\util\pyfiles.py", line 109, in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 994, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "C:\Users\trish1u\Desktop\autobi_real\backend\alembic\env.py", line 78, in <module>
    run_migrations_online()
  File "C:\Users\trish1u\Desktop\autobi_real\backend\alembic\env.py", line 60, in run_migrations_online
    connectable = engine_from_config(
                  ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\sqlalchemy\engine\create.py", line 820, in engine_from_config
    return create_engine(url, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in create_engine
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\sqlalchemy\util\deprecations.py", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\sqlalchemy\engine\create.py", line 550, in create_engine
    entrypoint = u._get_entrypoint()
    entrypoint = u._get_entrypoint()
                 ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\sqlalchemy\engine\url.py", line 758, in _get_entrypoint
    cls = registry.load(name)
          ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_real\backend\env\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 375, in load  
    raise exc.NoSuchModuleError(
sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:driver








Knowledge repo

from fastapi import FastAPI, HTTPException, Depends
from sqlmodel import Session, select
from typing import List
from datetime import datetime
from uuid import UUID

app = FastAPI()

# Dependency to get the session
def get_session():
    with Session(engine) as session:
        yield session

@app.get("/documents/", response_model=List[KnowledgeRepository])
def get_documents(tag: Optional[str] = None, status: Optional[DocumentStatus] = None, session: Session = Depends(get_session)):
    query = select(KnowledgeRepository)
    if tag:
        query = query.where(KnowledgeRepository.tags.contains([tag]))
    if status:
        query = query.where(KnowledgeRepository.progress == status)
    documents = session.exec(query).all()
    return documents

@app.post("/documents/", response_model=KnowledgeRepository)
def create_document(document: KnowledgeRepository, session: Session = Depends(get_session)):
    session.add(document)
    session.commit()
    session.refresh(document)
    return document

@app.put("/documents/{document_id}", response_model=KnowledgeRepository)
def update_document(document_id: UUID, updated_doc: KnowledgeRepository, session: Session = Depends(get_session)):
    document = session.get(KnowledgeRepository, document_id)
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    for key, value in updated_doc.dict(exclude_unset=True).items():
        setattr(document, key, value)
    document.last_modified = datetime.now(timezone.utc)  # Update modified timestamp
    session.add(document)
    session.commit()
    session.refresh(document)
    return document

@app.delete("/documents/{document_id}", response_model=dict)
def delete_document(document_id: UUID, session: Session = Depends(get_session)):
    document = session.get(KnowledgeRepository, document_id)
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    document.progress = DocumentStatus.DELETED
    session.add(document)
    session.commit()
    return {"message": "Document deleted"}










main.py

from fastapi import APIRouter, FastAPI
from fastapi.middleware.cors import CORSMiddleware
import sys

from app.api.routes import add_user, login,llm_config, organization, domain_request, role_request, smtp_handler,ldap_config, workspace, filter_workspace

api_router = APIRouter()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

api_router.include_router(login.router, tags=["login"])
api_router.include_router(add_user.router, tags=["users"])
api_router.include_router(llm_config.router,tags=["llmconfig"])
api_router.include_router(organization.router,tags=["organization"])
api_router.include_router(domain_request.router, tags=["domain"])
api_router.include_router(smtp_handler.router, tags=["smtp configuration"])
api_router.include_router(role_request.router, tags=["Role Change Request"])
api_router.include_router(ldap_config.router, tags=["ldap"])
api_router.include_router(workspace.router, tags=["homepage"])
api_router.include_router(filter_workspace.router, tags=["filter"])
# api.router.include_router(knowledge.router, tags=["knowledge"])


app.include_router(api_router)








models.py

from datetime import datetime, timezone
from typing import Optional,List
from uuid import UUID, uuid4
from enum import Enum
from sqlalchemy import JSON, Column,UniqueConstraint
from sqlmodel import Field, Relationship, SQLModel
import enum

class Token(SQLModel):
    access_token: str
    token_type: str = "bearer"

   
class Role(str, Enum):
    WORKSPACE_ADMIN = "workspace_admin"
    DOMAIN_STEWARD = "domain_admin"
    API_USER = "api_user"
    

# Association Table for User and Workspace with role and domains
class UserWorkspaceLink(SQLModel, table=True):
    __tablename__ = "user_workspace_link"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    role: Role  # Role can be 'workspace_admin', 'domain_admin', or 'api_user'
    domain_mapping_id: UUID = Field(default_factory=uuid4, foreign_key="user_workspace_domain_link.user_workspace_id", index = True)


# Association Table for favorited workspaces
class UserWorkspaceFavorited(SQLModel, table=True):
    __tablename__ = "user_workspace_favorited"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    favorited_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


# User Base Model
class UserBase(SQLModel):
    full_name: str
    username: str
    email: str
    functional_role: str  # Role like 'super_user' or 'application_user'
    is_active: bool = True
    is_super_user: bool = False


# User Table Model
class User(UserBase, table=True):
    __tablename__ = "user"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    hashed_password: str
    created: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated: datetime = datetime.now(timezone.utc)
    is_first_login: bool = True

    # Relationships
    workspaces: list["Workspace"] = Relationship(
        back_populates="users", link_model=UserWorkspaceLink
    )
    # Favorited workspaces
    favorited_workspaces: list["Workspace"] = Relationship(
        back_populates="favorited_by", link_model=UserWorkspaceFavorited
    )

    requests_raised: list["RequestAccess"] = Relationship(back_populates="user", sa_relationship_kwargs=dict(foreign_keys="[RequestAccess.user_id]"))



# Pydantic Models for login
class UserLogin(SQLModel):
    username: str
    password: str
    vz_domain: str | None = None
    method: str


# Pydantic Models for User
class UserCreate(UserBase):
    password: str


class UserRead(UserBase):
    id: UUID
    created: datetime
    last_updated: datetime 
    is_first_login: bool


class Tag(SQLModel, table= True):
    __tablename__ = "tag"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    object_type: str =Field(nullable= False)
    object_id: UUID = Field(nullable= False)
    tag_name: str = Field(nullable=False)
    created_date: datetime= Field(default_factory=datetime.utcnow)

    __mapper_args__ = {
        "polymorphic_on":"object_type"
    }
    

# Workspace Base Model
class WorkspaceBase(SQLModel):
    description: str 
    name: str
    default_llm_provider: str
    default_embedding_provider: str 
    default_embedding_model: str 
    default_llm_model: str 
    is_public: bool = False
    is_active : bool = True


# Workspace Table Model
class Workspace(WorkspaceBase, table=True):
    __tablename__ = "workspace"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    organization_id: UUID = Field(default_factory=uuid4,foreign_key="admin_org_settings.id")
    created_by_id: UUID = Field(default_factory=uuid4, foreign_key="user.id")
    created_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated_date: datetime = datetime.now(timezone.utc)
    request_access: list["RequestAccess"] = Relationship(back_populates="workspace")

    # Relationships
    tags : list["Tag"] = Relationship(sa_relationship_kwargs=
                                {"primaryjoin":"and_(Tag.object_id == Workspace.id, Tag.object_type == 'workspace')",
                                 "foreign_keys":"Tag.object_id","viewonly":True})
    
    users: list["User"] = Relationship(back_populates="workspaces", link_model=UserWorkspaceLink
    )
    favorited_by: list["User"] = Relationship(back_populates="favorited_workspaces", link_model=UserWorkspaceFavorited
    )

    domains: list["Domain"] = Relationship(back_populates="workspace")

    datasources: list["Datasource"] = Relationship(back_populates = "workspace")

# Pydantic Models for Workspace
class WorkspaceCreate(WorkspaceBase):
    tags: list[str] | None = None  # Include 'tags' here for input
    organization_id: UUID = Field(default_factory=uuid4) 
    
# Pydantic Models for creating a new Workspace
class NewWorkspaceRead(WorkspaceCreate):
    user_id: UUID = Field(default_factory=uuid4) 

# AdminOrgSettings Base Model
class AdminOrgSettingsBase(SQLModel):
    name: str
    description: str 


# AdminOrgSettings Table Model
class AdminOrgSettings(AdminOrgSettingsBase, table=True):
    __tablename__ = "admin_org_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminOrgSettings
class AdminOrgSettingsCreate(AdminOrgSettingsBase):
    pass


class AdminOrgSettingsRead(AdminOrgSettingsBase):
    id: UUID


# AdminLDAPSettings Base Model
class AdminLDAPSettingsBase(SQLModel):
    ldap_name: str = Field(index = True, unique = True)
    ldap_server: str
    ldap_port: int
    ldap_base_dn: str
    ldap_base_search: str
    ldap_ssl_enabled: bool = False
    ldap_bind_password: str
    ldap_search_filter: str

# AdminLDAPSettings Table Model
class AdminLDAPSettings(AdminLDAPSettingsBase, table=True):
    __tablename__ = "admin_ldap_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLDAPSettings
class AdminLDAPSettingsCreate(AdminLDAPSettingsBase):
    pass


class AdminLDAPSettingsRead(AdminLDAPSettingsBase):
    id: UUID


# AdminLLMProviderSettings Base Model
class AdminLLMProviderSettingsBase(SQLModel):
    llm_provider_name: str

class AdminLLMProviderSettings(AdminLLMProviderSettingsBase, table=True):
    __tablename__ = 'adminllmprovidersettings'
    __table_args__ = (UniqueConstraint('llm_provider_name', name='uix_llm_provider_name'),)
    id: UUID = Field(default_factory=uuid4, primary_key=True)

class AdminLLMProviderSettingsCreate(AdminLLMProviderSettingsBase):
    pass

class AdminLLMProviderSettingsRead(AdminLLMProviderSettingsBase):
    id: UUID

class AdminLLMModelSettingsBase(SQLModel):
    llm_model_name: str
    provider_id: UUID = Field(foreign_key="adminllmprovidersettings.id")
    llm_base_endpoint_url: Optional[str] = None
    llm_api_token: Optional[str] = None

class AdminLLMModelSettings(AdminLLMModelSettingsBase, table=True):
    __tablename__ = 'adminllmmodelsettings'
    __table_args__ = (
        UniqueConstraint('llm_model_name', 'provider_id', name='uix_llm_model_provider'),
    )
    id: UUID = Field(default_factory=uuid4, primary_key=True)

class AdminLLMModelSettingsCreate(AdminLLMModelSettingsBase):
    pass

class AdminLLMModelSettingsUpdate(SQLModel):
    llm_base_endpoint_url: Optional[str] = None
    llm_api_token: Optional[str] = None

class AdminLLMModelSettingsRead(AdminLLMModelSettingsBase):
    id: UUID

class OrganizationCreate(SQLModel):
    name: str = Field(max_length=50, unique=True, index=True, nullable=False)
    description: str


#SMTP Configuration
class SMTPConfigurationBase(SQLModel):
    smtp_server: str
    smtp_port: int
    smtp_username: str
    smtp_password: Optional[str] = None
    enable_tls: Optional[bool] = False
    email_from: str

class SMTPConfiguration(SMTPConfigurationBase, table=True):
    __tablename__ = "smtp_configuration"
    id: UUID = Field(default_factory=uuid4, primary_key = True)
    certification_id: Optional[UUID] = Field(default=None, foreign_key="smtp_certificate.id")

    certificate: Optional["SMTPCertificate"] = Relationship(back_populates="configuration")

class SMTPConfigurationCreate(SMTPConfigurationBase):
    certificate_id: Optional[UUID] = None

class SMTPConfigurationUpdate(SQLModel):
    smtp_server: Optional[str] = None
    smtp_port: Optional[int] = None
    smtp_username: Optional[str] = None
    smtp_password: Optional[str] = None
    enable_tls: Optional[bool] = None
    email_from: Optional[str] = None
    certificate_id: Optional[UUID] = None

class SMTPCertificate(SQLModel, table=True):
    __tablename__ = "smtp_certificate"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    filename: str 
    fileblob: bytes  
    uploaded_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc)) 

    configuration: Optional[SMTPConfiguration] = Relationship(back_populates="certificate")


#Response Model for uploading SMTP certificate
class SMTPCertificateOut(SQLModel):
    id: UUID = Field(default_factory=uuid4)
    filename: str 
    uploaded_at: datetime 


# Enum for Request Status
class RequestStatus(str, Enum):
    WAITING_APPROVAL = "WAITING_APPROVAL"
    APPROVED = "APPROVED"
    REJECTED = "REJECTED"
    REVOKED = "REVOKED"

# Base Model (Shared Fields)
class RequestAccessBase(SQLModel):
    user_id: UUID = Field(foreign_key="user.id")
    workspace_id: UUID = Field(foreign_key="workspace.id")
    role: Role
    reason: str = Field(nullable=False)
    status: Optional[RequestStatus] = Field(default=RequestStatus.WAITING_APPROVAL)


# RoleRequestCreate (For Creating New Requests)
class RequestAccessCreate(RequestAccessBase):
    domains: List[UUID]


# RoleRequest Model (Database Table)
class RequestAccess(RequestAccessBase, table=True):
    __tablename__ = "request_access"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    handled_by_user_id : Optional[UUID] = Field(default=None, foreign_key="user.id")
    # Relationships
    user: User = Relationship(sa_relationship_kwargs=dict(foreign_keys="[RequestAccess.user_id]"))
    workspace: Workspace = Relationship(back_populates="request_access")
    requested_domains: List["DomainRequestAccess"] = Relationship(back_populates="request_access", cascade_delete=True)
    handled_by_user: Optional["User"] = Relationship(sa_relationship_kwargs=dict(foreign_keys = "[RequestAccess.handled_by_user_id]"))


class DomainDatasourceLink(SQLModel, table = True):
    __tablename__ = "domain_datasource_link"
    domain_id: UUID = Field(foreign_key="domain.id",primary_key=True)
    datasource_id: UUID = Field(foreign_key= "datasource.id",primary_key = True)
    context_definition: str

  
class DatasourceBase(SQLModel):
    name: str
    technology_type: str
    

class Datasource(DatasourceBase,table = True):
    __tablename__ = "datasource"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    
    workspace_id : UUID = Field(foreign_key = "workspace.id")
    tags : list["Tag"] = Relationship(sa_relationship_kwargs=
                                {"primaryjoin":"and_(Tag.object_id == Datasource.id, Tag.object_type == 'datasource')",
                                 "foreign_keys":"Tag.object_id","viewonly":True})
    
    domains: list["Domain"] = Relationship(back_populates = "datasources",link_model = DomainDatasourceLink)
    workspace: Workspace = Relationship(back_populates = "datasources")

   
class DomainBase(SQLModel):
    name: str
    description: str
    technology_type: str

class Domain(DomainBase, table = True):
    __tablename__ = "domain"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    workspace_id: UUID = Field(foreign_key = "workspace.id")
    request_access: list["DomainRequestAccess"] = Relationship(back_populates="domains")
    
    workspace: Workspace = Relationship(back_populates = "domains")
    tags : list["Tag"] = Relationship(sa_relationship_kwargs=
                                {"primaryjoin":"and_(Tag.object_id == Domain.id, Tag.object_type == 'domain')",
                                 "foreign_keys":"Tag.object_id","viewonly":True})
    
    datasources: list["Datasource"] = Relationship(back_populates = "domains", link_model = DomainDatasourceLink )
    #TO-DO knowledge docs and prompts

class DomainCreate(DomainBase):
    workspace_id: UUID = Field(default_factory=uuid4)

class DomainRequestAccess(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    request_access_id: UUID = Field(foreign_key='request_access.id')
    domain_id: UUID = Field(foreign_key='domain.id')
    request_access: RequestAccess = Relationship(back_populates='requested_domains')
    domains: Domain = Relationship(back_populates='request_access')

class UserWorkspaceDomainLink(SQLModel, table=True):
    __tablename__ = "user_workspace_domain_link"
    user_workspace_id: UUID = Field(foreign_key="user_workspace_link.domain_mapping_id", primary_key=True)
    domain_id: UUID = Field(foreign_key="domain.id", primary_key=True)


class KnowledgeRepository(SQLModel, table =True):
    __tablename__ = "knowledge_repository"
    id: UUID = Field(default_factory=uuid4, primary_key= True)
    name: str
    description:Optional[str]=None
    workspace_id: UUID = Field(foreign_key="workspace.id")
    tags: list[str] = Field(sa_column= Column(JSON))
    index_reference_id: Optional[UUID] =None
    chunk_size: Optional[int] = None
    chunk_overlap: Optional[int]= None
    created_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    created_by:Optional[UUID] = Field(foreign_key="user.id")
    last_updated_by: Optional[UUID]= Field(foreign_key="user.id")
#     tags : list["Tag"] = Relationship(sa_relationship_kwargs=
#                             {"primaryjoin":"and_(Tag.object_id == KnowledgeRepository.id, Tag.object_type == 'knowledgerepo')",
#                                  "foreign_keys":"Tag.object_id","viewonly":True})
    

    
    workspaces: Optional["Workspace"] = Relationship(
        back_populates="knowledge_repositories"
    )

# KnowledgeRepository.__mapper_args__ = {
#     "polymorphic_identity": "knowledgerepo",
#     "with_polymorphic":"*"
# }


class DocumentStatus(str, Enum):
    IN_PROGRESS ="In Progress"
    COMPLETED = "Completed"
    FAILED = "Failed"
    DELETED = "Deleted"

Tag.__mapper_args__.update({
    "polymorphic_identity": "tag",
    "with_polymorphic":"*"
})

Workspace.__mapper_args__ = {
    "polymorphic_identity": "workspace",
    "with_polymorphic":"*"
}

Domain.__mapper_args__ = {
    "polymorphic_identity": "domain",
    "with_polymorphic":"*"
}

Datasource.__mapper_args__ = {
    "polymorphic_identity": "datasource",
    "with_polymorphic":"*"
}




knowledge_repo.py

from fastapi import FastAPI, HTTPException, Depends, APIRouter
from sqlmodel import Session, select, SQLModel, create_engine
from typing import List
from datetime import datetime
from uuid import UUID
from app.models import *
from app.api.deps import *

router = APIRouter()

# engine = create_engine(DATABASE_URL, echo= True)
# Dependency to get the session
# def get_session():
#     with Session(engine) as session:
#         yield session

@router.get("/documents/", response_model=List[KnowledgeRepository])
def get_documents(tag: Optional[str] = None, status: Optional[DocumentStatus] = None, session: Session = Depends(get_db)):
    query = select(KnowledgeRepository)
    if tag:
        query = query.where(KnowledgeRepository.tags.contains([tag]))
    if status:
        query = query.where(KnowledgeRepository.progress == status)
    documents = session.exec(query).all()
    return documents

@router.post("/documents/", response_model=KnowledgeRepository)
def create_document(document: KnowledgeRepository, session: Session = Depends(get_db)):
    new_repo= KnowledgeRepository(name=document.name, 
                                   description=document.description,
                                    workspace_id=document.workspace_id,
                                    tags= document.tags
                                   )
    session.add(new_repo)
    session.commit()
    session.refresh(new_repo)
    return document

@router.put("/documents/{document_id}", response_model=KnowledgeRepository)
def update_document(document_id: UUID, updated_doc: KnowledgeRepository, session: Session = Depends(get_db)):
    document = session.get(KnowledgeRepository, document_id)
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    for key, value in updated_doc.dict(exclude_unset=True).items():
        setattr(document, key, value)
    document.last_modified = datetime.now(timezone.utc)  # Update modified timestamp
    session.add(document)
    session.commit()
    session.refresh(document)
    return document

@router.delete("/documents/{document_id}", response_model=dict)
def delete_document(document_id: UUID, session: Session = Depends(get_db)):
    document = session.get(KnowledgeRepository, document_id)
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    document.progress = DocumentStatus.DELETED
    session.add(document)
    session.commit()
    return {"message": "Document deleted"}



main.py

from fastapi import APIRouter, FastAPI
from fastapi.middleware.cors import CORSMiddleware
import sys

from app.api.routes import add_user, login,llm_config, organization, domain_request, role_request, smtp_handler,ldap_config, workspace, filter_workspace, knowledge_repo

api_router = APIRouter()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

api_router.include_router(login.router, tags=["login"])
api_router.include_router(add_user.router, tags=["users"])
api_router.include_router(llm_config.router,tags=["llmconfig"])
api_router.include_router(organization.router,tags=["organization"])
api_router.include_router(domain_request.router, tags=["domain"])
api_router.include_router(smtp_handler.router, tags=["smtp configuration"])
api_router.include_router(role_request.router, tags=["Role Change Request"])
api_router.include_router(ldap_config.router, tags=["ldap"])
api_router.include_router(workspace.router, tags=["homepage"])
api_router.include_router(filter_workspace.router, tags=["filter"])
api_router.include_router(knowledge_repo.router, tags=["knowledge"])


app.include_router(api_router)





error

INFO:     127.0.0.1:59606 - "POST /documents/ HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2520, in get_property
    return self._props[key]
           ~~~~~~~~~~~^^^^^
KeyError: 'knowledge_repositories'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\uvicorn\protocols\http\httptools_impl.py", line 426, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\uvicorn\middleware\proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\fastapi\applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\applications.py", line 123, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\middleware\errors.py", line 186, in __call__
    raise exc
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\middleware\cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\_exception_handler.py", line 64, in wrapped_app
    raise exc
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\routing.py", line 762, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\routing.py", line 782, in app     
    await route.handle(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\routing.py", line 297, in handle  
    await self.app(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\routing.py", line 77, in app      
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\_exception_handler.py", line 64, in wrapped_app
    raise exc
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\starlette\routing.py", line 72, in app      
    response = await func(request)
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\fastapi\routing.py", line 285, in app       
    raise e
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\fastapi\routing.py", line 275, in app       
    solved_result = await solve_dependencies(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\fastapi\dependencies\utils.py", line 626, in solve_dependencies
    ) = await request_body_to_args(  # body_params checked above
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\fastapi\dependencies\utils.py", line 756, in request_body_to_args
    v_, errors_ = field.validate(value, values, loc=loc)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\fastapi\_compat.py", line 125, in validate  
    self._type_adapter.validate_python(value, from_attributes=True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\pydantic\type_adapter.py", line 208, in validate_python
    return self.validator.validate_python(__object, strict=strict, from_attributes=from_attributes, context=context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 4, in __init__
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\state.py", line 566, in _initialize_instance
    manager.dispatch.init(self, args, kwargs)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\event\attr.py", line 497, in __call__
    fn(*args, **kw)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 4407, in _event_on_init
    instrumenting_mapper._check_configure()
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2399, in _check_configure
    _configure_registries({self.registry}, cascade=True)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 4215, in _configure_registries
    _do_configure_registries(registries, cascade)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 4256, in _do_configure_registries
    mapper._post_configure_properties()
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2416, in _post_configure_properties
    prop.init()
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\interfaces.py", line 589, in init
    self.do_init()
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\relationships.py", line 1647, in do_init
    self._generate_backref()
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\relationships.py", line 2133, in _generate_backref
    self._add_reverse_property(self.back_populates)
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\relationships.py", line 1578, in _add_reverse_property
    other = self.mapper.get_property(key, _configure_mappers=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2522, in get_property
    raise sa_exc.InvalidRequestError(
sqlalchemy.exc.InvalidRequestError: Mapper 'Mapper[Workspace(workspace)]' has no property 'knowledge_repositories'.  If this property was indicated from other mappers or configure events, ensure registry.configure() has been called.
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2522, in get_property
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2522, in get_property
    raise sa_exc.InvalidRequestError(
sqlalchemy.exc.InvalidRequestError: Mapper 'Mapper[Workspace(workspace)]' has no property 'knowledge_repositories'.  If this property was indicated from other mappers or configure events, ensure registry.configure() has been called.
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2522, in get_property
    raise sa_exc.InvalidRequestError(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2522, in get            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\autobi_kr\backend\autobi_venv\Lib\site-packages\sqlalchemy\orm\mapper.py", line 2522, in get_property
    raise sa_exc.InvalidRequestError(
sqlalchemy.exc.InvalidRequestError: Mapper 'Mapper[Workspace(workspace)]' has no property 'knowledge_repositories'.  If this property was indicated from other mappers or configure events, ensure registry.configure() has been called.




























redis\

import redis

redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)



def get_data_from_cache(key):
    # Check if data is in Redis cache
    cached_data = redis_client.get(key)
    if cached_data:
        return cached_data
    
    # If not in cache, query the database
    data = query_database(key)
    
    # Store the data in cache with an expiration time
    redis_client.setex(key, 3600, data)  # Cache for 1 hour
    return data


def update_data_in_db_and_cache(key, new_data):
    # Update the database
    update_database(key, new_data)
    
    # Update the cache
    redis_client.setex(key, 3600, new_data)



import redis

# Initialize Redis client
redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)

def get_data(key):
    # Check Redis cache
    cached_data = redis_client.get(key)
    if cached_data:
        return cached_data
    
    # If data is not in cache, query database (simulate with a function call)
    data = query_database(key)
    
    # Cache the data with an expiration time of 1 hour
    redis_client.setex(key, 3600, data)
    
    return data

def query_database(key):
    # Simulate a database query
    # This function should actually fetch data from the database
    return f"Data for {key}"






import redis
import json

# Initialize Redis connection
redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)

# Function to cache data in Redis
def cache_data(api_key, user_id, domain_ids, workspace_id, expiration=3600):
    # Create the data dictionary
    data = {
        "user_id": user_id,
        "domain_id": domain_ids,
        "workspace_id": workspace_id
    }
    # Convert data to JSON string
    json_data = json.dumps(data)
    # Store in Redis with expiration
    redis_client.setex(api_key, expiration, json_data)
    print(f"Data cached for api_key: {api_key}")

# Function to retrieve data from Redis
def get_cached_data(api_key):
    # Get data from Redis
    cached_data = redis_client.get(api_key)
    
    if cached_data:
        # Convert JSON string back to dictionary
        data = json.loads(cached_data)
        print(f"Data retrieved from cache for api_key: {api_key}")
        return data
    else:
        print("Data not found in cache")
        return None

# Function to update cached data in Redis
def update_cached_data(api_key, user_id=None, new_domain_id=None, workspace_id=None):
    # Retrieve current data
    data = get_cached_data(api_key)
    if data:
        # Update fields if new values are provided
        if user_id is not None:
            data["user_id"] = user_id
        if new_domain_id is not None:
            # Append new domain_id if provided
            data["domain_id"].append(new_domain_id)
        if workspace_id is not None:
            data["workspace_id"] = workspace_id

        # Convert updated data back to JSON and store in Redis
        redis_client.setex(api_key, 3600, json.dumps(data))
        print(f"Data updated in cache for api_key: {api_key}")
    else:
        print("Cannot update data - key not found in cache")

# Function to delete cached data from Redis
def delete_cached_data(api_key):
    # Delete the cached entry
    redis_client.delete(api_key)
    print(f"Data deleted from cache for api_key: {api_key}")

# Usage examples

# Cache some data
cache_data("your_api_key", "12345", ["domain1", "domain2"], "workspace1")

# Retrieve cached data
data = get_cached_data("your_api_key")
print(data)

# Update cached data
update_cached_data("your_api_key", new_domain_id="domain3")

# Retrieve updated data to check if the update was successful
updated_data = get_cached_data("your_api_key")
print(updated_data)

# Delete cached data
delete_cached_data("your_api_key")










from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import redis
import json

# Initialize Redis connection
redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)

# Initialize FastAPI app
app = FastAPI()

# Data model for input data
class CacheData(BaseModel):
    api_key: str
    user_id: str
    domain_id: list[str]
    workspace_id: str

# Endpoint to cache data
@app.post("/cache/")
async def cache_data(data: CacheData):
    # Convert data to JSON
    json_data = json.dumps(data.dict())
    # Store in Redis with expiration (1 hour)
    redis_client.setex(data.api_key, 3600, json_data)
    return {"message": "Data cached successfully", "api_key": data.api_key}

# Endpoint to retrieve cached data
@app.get("/cache/{api_key}")
async def get_cached_data(api_key: str):
    # Retrieve data from Redis
    cached_data = redis_client.get(api_key)
    if cached_data:
        data = json.loads(cached_data)
        return {"data": data}
    else:
        raise HTTPException(status_code=404, detail="Data not found in cache")

# Endpoint to update cached data
@app.put("/cache/{api_key}")
async def update_cached_data(api_key: str, user_id: str = None, new_domain_id: str = None, workspace_id: str = None):
    # Retrieve current data
    cached_data = redis_client.get(api_key)
    if not cached_data:
        raise HTTPException(status_code=404, detail="Data not found in cache")

    data = json.loads(cached_data)

    # Update fields if new values are provided
    if user_id is not None:
        data["user_id"] = user_id
    if new_domain_id is not None:
        data["domain_id"].append(new_domain_id)
    if workspace_id is not None:
        data["workspace_id"] = workspace_id

    # Save updated data back to Redis
    redis_client.setex(api_key, 3600, json.dumps(data))
    return {"message": "Data updated successfully", "api_key": api_key}

# Endpoint to delete cached data
@app.delete("/cache/{api_key}")
async def delete_cached_data(api_key: str):
    # Delete the cached entry
    result = redis_client.delete(api_key)
    if result == 1:
        return {"message": "Data deleted successfully", "api_key": api_key}
    else:
        raise HTTPException(status_code=404, detail="Data not found in cache")



ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\connection.py", line 730, in _connect
    raise err
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\uvicorn\protocols\http\h11_impl.py", line 406, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\uvicorn\middleware\proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\fastapi\applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\middleware\errors.py", line 187, in __call__
    raise exc
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\middleware\errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\middleware\exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\_exception_handler.py", line 62, in wrapped_app
    raise exc
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\_exception_handler.py", line 51, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\routing.py", line 735, in app   
    await route.handle(scope, receive, send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\routing.py", line 76, in app    
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\_exception_handler.py", line 62, in wrapped_app
    raise exc
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\_exception_handler.py", line 51, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\starlette\routing.py", line 73, in app    
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\fastapi\routing.py", line 301, in app     
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\fastapi\routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\Desktop\redis\redisf.py", line 25, in cache_data
    redis_client.setex(data.api_key, 3600, json_data)
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\commands\core.py", line 2360, in setex
    return self.execute_command("SETEX", name, time, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\connection.py", line 1422, in get_connection
nnection
    connection.connect()
  File "C:\Users\trish1u\AppData\Local\Programs\Python\Python312\Lib\site-packages\redis\connection.py", line 363, in connect    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. No connection could be made because the target machine actively refused it.













rediss

from fastapi import APIRouter, Depends,HTTPException,status
from sqlmodel import select
from app.models import (
    UserWorkspaceLink,APIKeyDomainLink,
    UserWorkspaceDomainLink,APIKey,Role,Domain,APIKeyUpdate,APIKeyCreate
)
import secrets
from datetime import datetime,timedelta,timezone
from app.api.deps import SessionDep,get_current_user
from uuid import UUID
router=APIRouter()

@router.post("/api_keys/")
def create_api_key_endpoint(
    api_key_create:APIKeyCreate,
    session:SessionDep,
    # current_user:User=Depends(get_current_user)
):
    user_workspace_mapping=session.exec(
        select(UserWorkspaceLink)
        .where(UserWorkspaceLink.user_id==api_key_create.user_id)
        .where(UserWorkspaceLink.workspace_id==api_key_create.workspace_id)
        .where(UserWorkspaceLink.role==Role.API_USER)
    ).first()

    if not user_workspace_mapping:
        raise HTTPException(status_code=403,detail="User does not have access to the workspace")
    
    # finding all the domains the user needs

    domains=session.exec(
        select(Domain)
        .where(Domain.id.in_(api_key_create.domain_ids))
    ).all()

    #checking if any domain is missing or not 

    if not domains or len(domains)!=len(api_key_create.domain_ids):
        raise HTTPException(status_code=400, detail="Invalid domain id's provided")

    available_domains=session.exec(
        select(Domain)
        .join(UserWorkspaceDomainLink,UserWorkspaceDomainLink.domain_id==Domain.id)
        .where(UserWorkspaceDomainLink.user_workspace_id==user_workspace_mapping.domain_mapping_id)
        .where(Domain.id.in_(api_key_create.domain_ids))
    ).all()

    if len(available_domains)!=len(api_key_create.domain_ids):
        raise HTTPException(status_code=403, detail="One or more domains are not available for the user in the workspace")
    
    #checking the api keys already exist for given domains,workspace and user
    existing_api_keys=session.exec(
        select(APIKey)
        .where(APIKey.user_id==api_key_create.user_id)
        .where(APIKey.workspace_id==api_key_create.workspace_id)
    ).all()

    for existing_api_key in existing_api_keys:
        existing_domain_ids={domain.id for domain in existing_api_key.domain}
        if set(api_key_create.domain_ids)==existing_domain_ids:
            raise HTTPException(status_code=400, detail="API key with the same domains already exists")
    
    #after validating create and append apikey to db
    key=secrets.token_urlsafe(32)

    api_key_append=APIKey(
        api_key=key,
        description=api_key_create.description,
        user_id=api_key_create.user_id,
        workspace_id=api_key_create.workspace_id,
        created_at=datetime.now(timezone.utc),
        modified_at=datetime.now(timezone.utc),
        email_notification=False,
        exp_date=datetime.now(timezone.utc)+timedelta(days=365)
    )

    session.add(api_key_append)
    session.commit()
    session.refresh(api_key_append)
    
    for domains in available_domains:
        data=APIKeyDomainLink(api_key_id=api_key_append.id,domain_id=domains.id)
        session.add(data)
        session.commit()
        session.refresh(data)

    return {"status_code":201,"message":"success","data":api_key_append}

@router.get("/api_key/")
def get_api_key_endpoint(
    session:SessionDep,
    # current_user:User=Depends(get_current_user)
):
    response=[]

    api_keys=session.exec(select(APIKey)).all()
    for api_key in api_keys:
        response.append({
            "description":api_key.description,
            "api_key":api_key.api_key,
            "domain":[session.exec(
        select(Domain.name)
        .join(APIKeyDomainLink,APIKeyDomainLink.domain_id==Domain.id).where(APIKeyDomainLink.api_key_id==api_key.id)
    ).all()],
    "id":api_key.id
        })
    
    return {"status_code":200,"message":"success","data":response}

@router.put('/api_key/')
def update_api_key_endpoint(api_key_id:UUID,user_id:UUID,api_key_update:APIKeyUpdate,session:SessionDep):
    api_key=session.exec(
        select(APIKey)
        .where(APIKey.user_id==user_id).where(APIKey.id==api_key_id)
    ).first()
    if not api_key:
        raise HTTPException(status_code=404,detail="API key not found")
    if api_key.user_id!=user_id:
        raise HTTPException(status_code=403,detail="You dont have permisions to update this key")
    if api_key_update.description is not None:
        api_key.description=api_key_update.description

    if api_key_update.domain_ids is not None:
        user_workspace_mapping=session.exec(
            select(UserWorkspaceLink)
            .where(UserWorkspaceLink.user_id==user_id)
            .where(UserWorkspaceLink.workspace_id==api_key.workspace_id)
            .where(UserWorkspaceLink.role==Role.API_USER)
            ).first()
        if not user_workspace_mapping:
            raise HTTPException(status_code=403,detail="User does not have access to workspace")
        available_domains=session.exec(
            select(Domain)
            .join(UserWorkspaceDomainLink,UserWorkspaceDomainLink.domain_id==Domain.id)
            .where(UserWorkspaceDomainLink.user_workspace_id==user_workspace_mapping.domain_mapping_id)
            .where(Domain.id.in_(api_key_update.domain_ids))
        ).all()

        if len(available_domains)!=len(api_key_update.domain_ids):
            raise HTTPException(status_code=403, detail="One or more domains are not available for the user in the workspace")
        existing_api_keys=session.exec(
        select(APIKey)
        .where(APIKey.user_id==user_id)
        .where(APIKey.workspace_id==api_key.workspace_id).where(APIKey.id!=api_key.id)
            ).all()
        for existing_api_key in existing_api_keys:
            existing_domain_ids={d.id for d in existing_api_key.domain}
            if set(api_key.domain)==existing_domain_ids:
                raise HTTPException(status_code=400, detail="API key with the same domains already exists")
        api_key.domain=available_domains
        api_key.modified_at=datetime.now(timezone.utc)
        session.add(api_key)
        session.commit()
        session.refresh(api_key)
        
        return {"status_code":200,"message":"success","data":api_key}

@router.delete('/api_key/',status_code=status.HTTP_204_NO_CONTENT)
def delete_api_key_endpoint(
    api_key_id:UUID,
    session:SessionDep
):
    api_key=session.get(APIKey,api_key_id)
    if not api_key:
        raise HTTPException(status_code=404, detail="API key not found")
    if api_key.user_id!=api_key_id:
        raise HTTPException(status_code=403, detail="You dont have permissions to delete this API key")
    
    session.delete(api_key)
    session .commit()






import redis
import json
from fastapi import APIRouter, Depends, HTTPException, status
from sqlmodel import select
from app.models import (
    UserWorkspaceLink, APIKeyDomainLink, 
    UserWorkspaceDomainLink, APIKey, Role, Domain, APIKeyUpdate, APIKeyCreate
)
import secrets
from datetime import datetime, timedelta, timezone
from app.api.deps import SessionDep, get_current_user
from uuid import UUID

# Configure Redis client
redis_client = redis.Redis(host='localhost', port=6379, db=0)

router = APIRouter()

def cache_api_key(api_key, user_id, domain_ids, workspace_id):
    # Convert domain_ids list to a JSON serializable format
    cache_data = {
        "user_id": str(user_id),
        "domain_ids": [str(domain) for domain in domain_ids],
        "workspace_id": str(workspace_id)
    }
    redis_client.set(api_key, json.dumps(cache_data))

@router.post("/api_keys/")
def create_api_key_endpoint(
    api_key_create: APIKeyCreate,
    session: SessionDep,
):
    # Check for user-workspace mapping
    user_workspace_mapping = session.exec(
        select(UserWorkspaceLink)
        .where(UserWorkspaceLink.user_id == api_key_create.user_id)
        .where(UserWorkspaceLink.workspace_id == api_key_create.workspace_id)
        .where(UserWorkspaceLink.role == Role.API_USER)
    ).first()

    if not user_workspace_mapping:
        raise HTTPException(status_code=403, detail="User does not have access to the workspace")
    
    # Check and validate domains
    domains = session.exec(
        select(Domain)
        .where(Domain.id.in_(api_key_create.domain_ids))
    ).all()
    if not domains or len(domains) != len(api_key_create.domain_ids):
        raise HTTPException(status_code=400, detail="Invalid domain id's provided")

    available_domains = session.exec(
        select(Domain)
        .join(UserWorkspaceDomainLink, UserWorkspaceDomainLink.domain_id == Domain.id)
        .where(UserWorkspaceDomainLink.user_workspace_id == user_workspace_mapping.domain_mapping_id)
        .where(Domain.id.in_(api_key_create.domain_ids))
    ).all()

    if len(available_domains) != len(api_key_create.domain_ids):
        raise HTTPException(status_code=403, detail="One or more domains are not available for the user in the workspace")
    
    # Check for existing API keys
    existing_api_keys = session.exec(
        select(APIKey)
        .where(APIKey.user_id == api_key_create.user_id)
        .where(APIKey.workspace_id == api_key_create.workspace_id)
    ).all()

    for existing_api_key in existing_api_keys:
        existing_domain_ids = {domain.id for domain in existing_api_key.domain}
        if set(api_key_create.domain_ids) == existing_domain_ids:
            raise HTTPException(status_code=400, detail="API key with the same domains already exists")

    # Generate API key and save to DB
    key = secrets.token_urlsafe(32)
    api_key_append = APIKey(
        api_key=key,
        description=api_key_create.description,
        user_id=api_key_create.user_id,
        workspace_id=api_key_create.workspace_id,
        created_at=datetime.now(timezone.utc),
        modified_at=datetime.now(timezone.utc),
        email_notification=False,
        exp_date=datetime.now(timezone.utc) + timedelta(days=365)
    )

    session.add(api_key_append)
    session.commit()
    session.refresh(api_key_append)
    
    for domain in available_domains:
        data = APIKeyDomainLink(api_key_id=api_key_append.id, domain_id=domain.id)
        session.add(data)
        session.commit()
        session.refresh(data)

    # Cache the new API key
    cache_api_key(api_key=key, user_id=api_key_create.user_id, domain_ids=api_key_create.domain_ids, workspace_id=api_key_create.workspace_id)

    return {"status_code": 201, "message": "success", "data": api_key_append}

@router.get("/api_key/")
def get_api_key_endpoint(session: SessionDep):
    response = []

    api_keys = session.exec(select(APIKey)).all()
    for api_key in api_keys:
        cached_data = redis_client.get(api_key.api_key)
        
        # Check Redis cache
        if cached_data:
            # If cached, use the cached data
            data = json.loads(cached_data)
        else:
            # If not cached, query the database and cache the result
            domain_names = session.exec(
                select(Domain.name)
                .join(APIKeyDomainLink, APIKeyDomainLink.domain_id == Domain.id)
                .where(APIKeyDomainLink.api_key_id == api_key.id)
            ).all()
            
            data = {
                "user_id": api_key.user_id,
                "workspace_id": api_key.workspace_id,
                "domain_ids": [domain.id for domain in api_key.domain]
            }
            cache_api_key(api_key.api_key, data["user_id"], data["domain_ids"], data["workspace_id"])

        response.append({
            "description": api_key.description,
            "api_key": api_key.api_key,
            "domains": data["domain_ids"],
            "id": api_key.id
        })
    
    return {"status_code": 200, "message": "success", "data": response}










import redis
import json
redis_client = redis.Redis(
    host= 'localhost',
    username= "user",
    password= "admin",
    port=6380, db= 0
)

redis_client.set("Hello World", "userr")


redis-server : The term 'redis-server' is not recognized as the name of a cmdlet, function, script file, or 
operable program. Check the spelling of the name, or if a path was included, verify that the path is 
correct and try again.
At line:1 char:1
+ redis-server
+ ~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (redis-server:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException























import requests

# Replace with your Upstash Redis REST URL and Token
UPSTASH_REDIS_REST_URL = "https://pet-amoeba-25520.upstash.io"  # Example URL; replace with yours
UPSTASH_REDIS_REST_TOKEN = "AWO...MA"  # Replace with your actual token

# Set up headers with the authorization token
headers = {
    "Authorization": f"Bearer {UPSTASH_REDIS_REST_TOKEN}",
    "Content-Type": "application/json"
}

# Function to set a key-value pair in Redis
def set_key(key, value):
    url = f"{UPSTASH_REDIS_REST_URL}/set/{key}/{value}"
    response = requests.get(url, headers=headers)
    return response.json()  # Returns JSON response from Upstash

# Function to get a value by key from Redis
def get_key(key):
    url = f"{UPSTASH_REDIS_REST_URL}/get/{key}"
    response = requests.get(url, headers=headers)
    return response.json()  # Returns JSON response from Upstash

# Example usage
try:
    # Set a key-value pair
    set_response = set_key("foo", "bar")
    print("Set response:", set_response)

    # Get the value of the key
    get_response = get_key("foo")
    print("Get response:", get_response)

except requests.exceptions.RequestException as e:
    print("An error occurred:", e)
























from datetime import datetime, timezone,timedelta
from typing import Optional,List
from uuid import UUID, uuid4
from enum import Enum
from sqlalchemy import JSON, Column,UniqueConstraint
from sqlmodel import Field, Relationship, SQLModel


class Token(SQLModel):
    access_token: str
    token_type: str = "bearer"

class APIKeyDomainLink(SQLModel,table=True):
    api_key_id:UUID=Field(foreign_key="api_key.id",primary_key=True)
    domain_id:UUID=Field(foreign_key="domain.id",primary_key=True)


class Role(str, Enum):
    WORKSPACE_ADMIN = "workspace_admin"
    DOMAIN_ADMIN = "domain_admin"
    API_USER = "api_user"


# Association Table for User and Workspace with role and domains
class UserWorkspaceLink(SQLModel, table=True):
    __tablename__ = "user_workspace_link"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    role: Role  # Role can be 'workspace_admin', 'domain_admin', or 'api_user'
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_activity : datetime = Field(default_factory=lambda: datetime.now(timezone.utc))    
    
    domain_mapping_id: UUID = Field(default_factory=uuid4, foreign_key="user_workspace_domain_link.user_workspace_id", index = True)
    
# Association Table for favorited workspaces
class UserWorkspaceFavorited(SQLModel, table=True):
    __tablename__ = "user_workspace_favorited"
    user_id: UUID = Field(foreign_key="user.id", primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id", primary_key=True)
    favorited_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


# User Base Model
class UserBase(SQLModel):
    full_name: str
    username: str
    email: str
    functional_role: str  # Role like 'super_user' or 'application_user'
    is_active: bool = True
    is_super_user: bool = False


# User Table Model
class User(UserBase, table=True):
    __tablename__ = "user"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    hashed_password: str
    created: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated: datetime = datetime.now(timezone.utc)
    is_first_login: bool = True
    
    # Relationships
    workspaces: list["Workspace"] = Relationship(
        back_populates="users", link_model=UserWorkspaceLink
    )
    # Favorited workspaces
    favorited_workspaces: list["Workspace"] = Relationship(
        back_populates="favorited_by", link_model=UserWorkspaceFavorited
    )

    requests_raised: list["RequestAccess"] = Relationship(back_populates="user", sa_relationship_kwargs=dict(foreign_keys="[RequestAccess.user_id]"))
    api_key:List["APIKey"]=Relationship(back_populates="user")

# Pydantic Models for login
class UserLogin(SQLModel):
    username: str
    password: str
    vz_domain: str | None = None
    method: str


# Pydantic Models for User
class UserCreate(UserBase):
    password: str


class UserRead(UserBase):
    id: UUID
    created: datetime
    last_updated: datetime 
    is_first_login: bool


class Tag(SQLModel, table= True):
    __tablename__ = "tag"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    object_type: str =Field(nullable= False)
    object_id: UUID = Field(nullable= False)
    tag_name: str = Field(nullable=False)
    created_date: datetime= Field(default_factory=datetime.utcnow)

    __mapper_args__ = {
        "polymorphic_on":"object_type"
    }


# Workspace Base Model
class WorkspaceBase(SQLModel):
    description: str 
    name: str
    default_llm_provider: UUID
    default_embedding_provider: UUID 
    default_embedding_model: UUID 
    default_llm_model: UUID 
    is_public: bool = False
    is_active : bool = True


# Workspace Table Model
class Workspace(WorkspaceBase, table=True):
    __tablename__ = "workspace"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    organization_id: UUID = Field(default_factory=uuid4,foreign_key="admin_org_settings.id")
    created_by_id: UUID = Field(default_factory=uuid4, foreign_key="user.id")
    created_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated_date: datetime = datetime.now(timezone.utc)
    
    # Relationships
    tags : list["Tag"] = Relationship(sa_relationship_kwargs=
                                {"primaryjoin":"and_(Tag.object_id == Workspace.id, Tag.object_type == 'workspace')",
                                 "foreign_keys":"Tag.object_id","viewonly":True})
    
    users: list["User"] = Relationship(back_populates="workspaces", link_model=UserWorkspaceLink)
    
    favorited_by: list["User"] = Relationship(back_populates="favorited_workspaces", link_model=UserWorkspaceFavorited)

    domains: list["Domain"] = Relationship(back_populates="workspace")

    datasources: list["Datasource"] = Relationship(back_populates = "workspace")
    
    request_access: list["RequestAccess"] = Relationship(back_populates="workspace")
    
    
    api_key:List["APIKey"]=Relationship(back_populates="workspace")


# Pydantic Models for Workspace
class WorkspaceCreate(WorkspaceBase):
    tags: list[str] | None = None  # Include 'tags' here for input
    organization_id: UUID = Field(default_factory=uuid4) 
    
    
class WorkspaceEdit(WorkspaceBase):
    tags: list[str] | None = None  # Include 'tags' here for input
    
    
class WorkspaceOut(WorkspaceBase):
    id:UUID
    description: str
    organization:Optional[str] = None
    created_date: datetime
    last_updated_date: datetime
    user_count: int
    domain_count: int
    datasource_count: int
    pending_approval_count: Optional[int] = None
    is_favourited: Optional[bool] = None
    tags: list[str]
    

# AdminOrgSettings Base Model
class AdminOrgSettingsBase(SQLModel):
    name: str
    description: str 


# AdminOrgSettings Table Model
class AdminOrgSettings(AdminOrgSettingsBase, table=True):
    __tablename__ = "admin_org_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminOrgSettings
class AdminOrgSettingsCreate(AdminOrgSettingsBase):
    pass


class AdminOrgSettingsRead(AdminOrgSettingsBase):
    id: UUID


# AdminLDAPSettings Base Model
class AdminLDAPSettingsBase(SQLModel):
    ldap_name: str = Field(index = True, unique = True)
    ldap_server: str
    ldap_port: int
    ldap_base_dn: str
    ldap_base_search: str
    ldap_ssl_enabled: bool = False
    ldap_bind_password: str
    ldap_search_filter: str

# AdminLDAPSettings Table Model
class AdminLDAPSettings(AdminLDAPSettingsBase, table=True):
    __tablename__ = "admin_ldap_settings"
    id: UUID = Field(default_factory=uuid4, primary_key=True)


# Pydantic Models for AdminLDAPSettings
class AdminLDAPSettingsCreate(AdminLDAPSettingsBase):
    pass


class AdminLDAPSettingsRead(AdminLDAPSettingsBase):
    id: UUID


# AdminLLMProviderSettings Base Model
class AdminLLMProviderSettingsBase(SQLModel):
    llm_provider_name: str

class AdminLLMProviderSettings(AdminLLMProviderSettingsBase, table=True):
    __tablename__ = 'adminllmprovidersettings'
    __table_args__ = (UniqueConstraint('llm_provider_name', name='uix_llm_provider_name'),)
    id: UUID = Field(default_factory=uuid4, primary_key=True)

class AdminLLMProviderSettingsCreate(AdminLLMProviderSettingsBase):
    pass

class AdminLLMProviderSettingsRead(AdminLLMProviderSettingsBase):
    id: UUID

class AdminLLMModelSettingsBase(SQLModel):
    llm_model_name: str
    provider_id: UUID = Field(foreign_key="adminllmprovidersettings.id")
    llm_base_endpoint_url: Optional[str] = None
    llm_api_token: Optional[str] = None

class AdminLLMModelSettings(AdminLLMModelSettingsBase, table=True):
    __tablename__ = 'adminllmmodelsettings'
    __table_args__ = (
        UniqueConstraint('llm_model_name', 'provider_id', name='uix_llm_model_provider'),
    )
    id: UUID = Field(default_factory=uuid4, primary_key=True)

class AdminLLMModelSettingsCreate(AdminLLMModelSettingsBase):
    pass

class AdminLLMModelSettingsUpdate(SQLModel):
    llm_base_endpoint_url: Optional[str] = None
    llm_api_token: Optional[str] = None

class AdminLLMModelSettingsRead(AdminLLMModelSettingsBase):
    id: UUID

class OrganizationCreate(SQLModel):
    name: str = Field(max_length=50, unique=True, index=True, nullable=False)
    description: str


# SMTP Configuration
class SMTPConfigurationBase(SQLModel):
    smtp_server: str
    smtp_port: int
    smtp_username: str
    smtp_password: Optional[str] = None
    enable_tls: Optional[bool] = False
    email_from: str

class SMTPConfiguration(SMTPConfigurationBase, table=True):
    __tablename__ = "smtp_configuration"
    id: UUID = Field(default_factory=uuid4, primary_key = True)
    certificate_id: Optional[UUID] = Field(default=None, foreign_key="smtp_certificate.id")

    certificate: Optional["SMTPCertificate"] = Relationship(back_populates="configuration")

class SMTPConfigurationCreate(SMTPConfigurationBase):
    certificate_id: Optional[UUID] = None

class SMTPConfigurationUpdate(SQLModel):
    smtp_server: Optional[str] = None
    smtp_port: Optional[int] = None
    smtp_username: Optional[str] = None
    smtp_password: Optional[str] = None
    enable_tls: Optional[bool] = None
    email_from: Optional[str] = None
    certificate_id: Optional[UUID] = None

class SMTPCertificate(SQLModel, table=True):
    __tablename__ = "smtp_certificate"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    filename: str 
    fileblob: bytes  
    uploaded_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc)) 

    configuration: Optional[SMTPConfiguration] = Relationship(back_populates="certificate")


# Response Model for uploading SMTP certificate
class SMTPCertificateOut(SQLModel):
    id: UUID = Field(default_factory=uuid4)
    filename: str 
    uploaded_at: datetime 


# Enum for Request Status
class RequestStatus(str, Enum):
    WAITING_APPROVAL = "WAITING_APPROVAL"
    APPROVED = "APPROVED"
    REJECTED = "REJECTED"
    REVOKED = "REVOKED"

# Base Model (Shared Fields)
class RequestAccessBase(SQLModel):
    role: Role
    reason: str = Field(nullable=False)
    status: Optional[RequestStatus] = Field(default=RequestStatus.WAITING_APPROVAL)
    handled_reason: Optional[str] = Field(default=None)
    full_domain_access: bool = Field(default=False)

# RequestAccessCreate (For Creating New Requests)
class RequestAccessCreate(RequestAccessBase):
    workspace_id: UUID = Field(foreign_key="workspace.id")
    domains: Optional[list[UUID]] = None


# RequestAccess Model (Database Table)
class RequestAccess(RequestAccessBase, table=True):
    __tablename__ = "request_access"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    user_id: UUID = Field(foreign_key="user.id")
    workspace_id: UUID = Field(foreign_key="workspace.id")
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    handled_by_user_id : Optional[UUID] = Field(default=None, foreign_key="user.id")
    # Relationships
    user: User = Relationship(sa_relationship_kwargs=dict(foreign_keys="[RequestAccess.user_id]"))
    workspace: Workspace = Relationship(back_populates="request_access")
    requested_domains: List["DomainRequestAccess"] = Relationship(back_populates="request_access", cascade_delete=True)
    handled_by_user: Optional["User"] = Relationship(sa_relationship_kwargs=dict(foreign_keys = "[RequestAccess.handled_by_user_id]"))

class RequestHandler(SQLModel):
    request_id: UUID
    handled_reason: Optional[str] = None

class RequestAccessOut(SQLModel):
    id: UUID
    user_id: UUID
    workspace_id: UUID
    workspace_name: str
    full_name: str
    username: str
    role: Role
    reason: str
    status: RequestStatus
    created_at: datetime
    updated_at: datetime
    full_domain_access: bool = False
    domain: Optional[list[UUID]] = None

class DomainDatasourceLink(SQLModel, table = True):
    __tablename__ = "domain_datasource_link"
    domain_id: UUID = Field(foreign_key="domain.id",primary_key=True)
    datasource_id: UUID = Field(foreign_key= "datasource.id",primary_key = True)
    context_definition: str

class DatasourceBase(SQLModel):
    name: str
    technology_type: str
    is_active : bool = True
    

class Datasource(DatasourceBase,table = True):
    __tablename__ = "datasource"
    id: UUID = Field(default_factory=uuid4, primary_key=True)

    workspace_id : UUID = Field(foreign_key = "workspace.id")
    tags : list["Tag"] = Relationship(sa_relationship_kwargs=
                                {"primaryjoin":"and_(Tag.object_id == Datasource.id, Tag.object_type == 'datasource')",
                                 "foreign_keys":"Tag.object_id","viewonly":True})

    domains: list["Domain"] = Relationship(back_populates = "datasources",link_model = DomainDatasourceLink)
    workspace: Workspace = Relationship(back_populates = "datasources")

class InviteDomainLink(SQLModel, table=True):
    __tablename__ = "invite_domain_link"
    invite_id: UUID = Field(foreign_key="invitations.id", primary_key=True)
    domain_id: UUID = Field(foreign_key="domain.id", primary_key=True)


class DomainBase(SQLModel):
    name: str
    description: str
    technology_type: str | None
    default_llm_provider: UUID | None = None
    default_embedding_provider: UUID | None = None
    default_embedding_model: UUID | None = None
    default_llm_model: UUID | None = None
    is_active: bool = True

class Domain(DomainBase, table=True):
    __tablename__ = "domain"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id")
    request_access: list["DomainRequestAccess"] = Relationship(back_populates="domains")
    workspace: Workspace = Relationship(back_populates="domains")
    tags: list["Tag"] = Relationship(
        sa_relationship_kwargs={
            "primaryjoin": "and_(Tag.object_id == Domain.id,Tag.object_type == 'domain')",
            "foreign_keys": "Tag.object_id",
            "viewonly": True,
        }
    )
    datasources: list["Datasource"] = Relationship(
        back_populates="domains", link_model=DomainDatasourceLink
    )
    invitations: list["Invitations"] = Relationship(back_populates="domains", link_model=InviteDomainLink)
    # TO-DO knowledge docs and prompts
    api_key:List["APIKey"]=Relationship(back_populates="domain",link_model=APIKeyDomainLink)

class DomainCreate(DomainBase):
    workspace_id: UUID = Field(default_factory=uuid4)
    tags: list[str] | None = None


class DomainRequestAccess(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    request_access_id: UUID = Field(foreign_key='request_access.id')
    domain_id: UUID = Field(foreign_key='domain.id')
    request_access: RequestAccess = Relationship(back_populates='requested_domains')
    domains: Domain = Relationship(back_populates='request_access')

class UserWorkspaceDomainLink(SQLModel, table=True):
    __tablename__ = "user_workspace_domain_link"
    user_workspace_id: UUID = Field(foreign_key="user_workspace_link.domain_mapping_id", primary_key=True)
    domain_id: UUID = Field(foreign_key="domain.id", primary_key=True)

# =================INVTATIONS MODEL=================

class Invitations(SQLModel, table=True):
    __tablename__ = "invitations"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    workspace_id: UUID = Field(foreign_key="workspace.id")
    invited_by_id: UUID = Field(foreign_key="user.id")
    role: Role
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    full_domain_access: bool = Field(default=False)

    invited_to: list["InviteTo"] = Relationship(back_populates="invite", cascade_delete=True)
    domains: Optional[list[Domain]] = Relationship(back_populates="invitations", link_model=InviteDomainLink)

class InviteTo(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    Invite_id: UUID = Field(foreign_key="invitations.id")
    username: str

    invite: Invitations = Relationship(back_populates="invited_to")

class InvitationCreate(SQLModel):
    workspace_id: UUID
    invited_by_id: UUID
    role: Role
    invitees: list[str]     #list of username
    domain_ids: Optional[list[UUID]] = None
    full_domain_access: bool = False

class InvitationUpdate(SQLModel):
    role: Role = None
    invitees: list[str] = None   #list of username
    domain_ids: Optional[list[UUID]] = None

#Pydantic models for members
class MemberUpdate(SQLModel):
    user_id: UUID
    workspace_id: UUID
    role: Optional[str] = None
    domain_ids: Optional[list[UUID]] = None

class MemberDelete(SQLModel):
    user_id: UUID
    workspace_id: UUID

class MemberOut(SQLModel):
    user_id: UUID
    username: str
    role:Role
    created_at: datetime
    last_updated_at:datetime
    last_activty: datetime
    domain_ids: Optional[list[UUID]]=None

Tag.__mapper_args__.update({
    "polymorphic_identity": "tag",
    "with_polymorphic":"*"
})

Workspace.__mapper_args__ = {
    "polymorphic_identity": "workspace",
    "with_polymorphic":"*"
}

Domain.__mapper_args__ = {
    "polymorphic_identity": "domain",
    "with_polymorphic":"*"
}

Datasource.__mapper_args__ = {
    "polymorphic_identity": "datasource",
    "with_polymorphic":"*"
}

class APIKeyBase(SQLModel):
    api_key:str=Field(index=True,unique=True)
    description:str
    
    
    
class APIKey(APIKeyBase,table=True):
    __tablename__="api_key"
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    user_id :UUID=Field(foreign_key="user.id")
    workspace_id:UUID=Field(foreign_key="workspace.id")
    user:"User"=Relationship(back_populates="api_key")
    workspace:"Workspace"=Relationship(back_populates="api_key")
    domain:List["Domain"]= Relationship(back_populates="api_key",link_model=APIKeyDomainLink)
    created_at:Optional[datetime]= Field(default_factory=lambda: datetime.now(timezone.utc))
    modified_at: Optional[datetime] = Field(default_factory=lambda: datetime.now(timezone.utc))
    email_notification:Optional[bool]=Field(default=False)
    exp_date:Optional[datetime]=Field(default_factory=lambda:datetime.now(timezone.utc)+timedelta(days=365))

class APIKeyCreate(SQLModel):
    description:str
    domain_ids:List[UUID] 
    workspace_id:UUID

class APIKeyRead(SQLModel):
    id:UUID
    api_key:str
    description:str
    user_id:UUID
    workspace_id:UUID 
    domain_ids:List[UUID]
    created_at:datetime
    modified_at:datetime
    email_notified:bool
    exp_date:datetime


class APIKeyUpdate(SQLModel):
    description:Optional[str]=None
    domain_ids:Optional[List[UUID]]=None


class DomainSearch(SQLModel):
    user_id:UUID
    workspace_id:UUID

class APIKeyCaching(SQLModel):
    id:UUID
    user_id:UUID
    workspace_id:UUID
    api_key_id:UUID
    domain_ids:List[UUID]
    api_key:str
    description:str









from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from enum import Enum
import time

app = FastAPI()

# Enum to represent the status
class ValidationStatus(str, Enum):
    valid = "Valid"
    invalid = "Invalid"
    in_progress = "In-Progress"

# Model to store knowledge base status
class KnowledgeBase(BaseModel):
    status: ValidationStatus = ValidationStatus.valid

knowledge_base = KnowledgeBase()

# Simulate a validation process
def validate_knowledge_base():
    time.sleep(5)  # Simulating a long-running validation process
    # Randomly set the status to valid or invalid
    from random import choice
    knowledge_base.status = choice([ValidationStatus.valid, ValidationStatus.invalid])

@app.get("/knowledge_base/status", response_model=KnowledgeBase)
def get_status():
    """
    Endpoint to get the current status of the knowledge base validation.
    """
    return knowledge_base

@app.post("/knowledge_base/refresh")
def refresh_knowledge_base(background_tasks: BackgroundTasks):
    """
    Endpoint to refresh the knowledge base validation.
    - Sets the status to 'In-Progress'.
    - Disables the refresh button by not allowing concurrent refresh requests.
    """
    if knowledge_base.status == ValidationStatus.in_progress:
        return {"message": "Validation already in progress. Please wait."}

    # Set status to In-Progress
    knowledge_base.status = ValidationStatus.in_progress

    # Start the validation process in the background
    background_tasks.add_task(validate_knowledge_base)
    return {"message": "Knowledge base validation started."}










from fastapi import APIRouter, Depends, Path, Query, HTTPException, FastAPI
from uuid import UUID
from typing import List, Optional
from sqlmodel import Session
from app.db import get_db

from app.crud.datasource import (
    soft_delete_datasource,
    search_tags,
    get_db_types,
    get_connection_names,
    search_datasources,
    refresh_metadata,
    update_metadata
)

router = APIRouter()
# router = FastAPI()

@router.put("/workspace/{workspace_id}/datasource/{datasource_id}/delete")
def soft_delete_workspace(workspace_id: UUID, datasource_id: UUID, db: Session = Depends(get_db)):
    success = soft_delete_datasource(db, datasource_id)
    if success:
        return {
            "status_code": 200,
            "message": "success",
            "data": str(datasource_id)
        }
    else:
        return {
            "status_code": 404,
            "message": "Datasource not found"
        }


@router.get("/workspace/{workspace_id}/datasource/tag/search")
def search_tags_endpoint(workspace_id: UUID,
                         search_text: Optional[str] = Query(None),
                         db: Session = Depends(get_db)):
    tags = search_tags(db, workspace_id, search_text)
    return {
        "status_code": 200,
        "message": "success",
        "data": tags
    }


@router.get("/workspace/{workspace_id}/datasource/db_types")
def get_db_types_endpoint(workspace_id: UUID, db: Session = Depends(get_db)):
    db_types = get_db_types(db, workspace_id)
    # Format as required: [{"db_type": name, "db_id": id}, ...]
    data = [{"db_type": name, "db_id": id_} for (name, id_) in db_types]
    return {
        "status_code": 200,
        "message": "success",
        "data": data
    }


@router.get("/workspace/{workspace_id}/datasource/{database_id}/connection_name")
async def get_connection_name_endpoint(
    workspace_id: UUID = Path(...),
    database_id: UUID = Path(...),
    search_text: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    conns = get_connection_names(db, workspace_id, database_id, search_text)
    data = [{"conn_name": c[0], "conn_id": c[1]} for c in conns]
    return {
        "status_code": 200,
        "message": "success",
        "data": data
    }


@router.get("/workspace/{workspace_id}/datasource/search")
async def search_datasource_endpoint(
    workspace_id: UUID = Path(...),
    search_text: Optional[str] = Query(None),
    connection_id: Optional[UUID] = Query(None),
    db_type: Optional[UUID] = Query(None),
    tags: Optional[str] = Query(None),
    page: int = Query(0),
    page_size: int = Query(6),
    db: Session = Depends(get_db)
):
    
    skip = page * page_size
    result, total = search_datasources(db, workspace_id, search_text, connection_id, db_type, tags, skip, page_size)
    return {
        "status_code": 200,
        "message": "success",
        "data": result,
        "count": total
    }


@router.get("/workspace/{workspace_id}/datasource/metadata_refresh/")
async def refresh_metadata_endpoint(
    workspace_id: UUID = Path(...),
    datasource_ids: str = Query(...),
    db: Session = Depends(get_db)
):
    try:
        id_lists = [UUID(x.strip()) for x in datasource_ids.split(",")]
    except Exception:
        return {
            "status_code": 404,
            "message": "Invalid UUIDs"
        }

    data = refresh_metadata(db, id_lists)
    if not data:
        return {
            "status_code": 404,
            "message": "No valid datasource found",
            "data": []
        }
    return {
        "status_code": 200,
        "message": "success",
        "data": data
    }


@router.post("/workspace/{workspace_id}/datasource/{datasource_id}/update_metadata/")
async def update_metadata_endpoint(workspace_id: UUID, datasource_id: UUID, db: Session = Depends(get_db)):
    success, msg = update_metadata(db, datasource_id)
    if success:
        return {
            "status_code": 200,
            "message": "success",
            "datasource_id": datasource_id
        }
    else:
        # If msg indicates a failure condition, we can return 400 or 404 accordingly.
        # For simplicity, let's return 400 if not found is indicated as well
        # In a real scenario, differentiate 404 not found vs. 400 bad request
        if "not found" in msg.lower():
            raise HTTPException(status_code=404, detail=msg)
        else:
            return {
                "status_code": 400,
                "message": "failed",
                "datasource_id": datasource_id
            }











db.py




from sqlmodel import create_engine, Session,select
from app.core.config import settings
from app.models import User,UserCreate
from app.core.auth.security import get_password_hash

DATABASE_URL = "postgresql+psycopg2://swarm:swarm123@autobi2-postgres-service:5432/swarm"
# DATABASE_URL = "sqlite:////app/autobi.db"
engine = create_engine(DATABASE_URL, echo=True)


FIRST_SUPERUSER="golla.narendra@verizon.com"
FIRST_SUPERUSER_PASSWORD="Incedo&Verizon"
def init_db(session: Session) -> None:
    existing_user = session.exec(
        select(User).where(
            (User.username == "adminautobi") | (User.email == FIRST_SUPERUSER)
        )
    ).first()
    if not existing_user:
         # Hash the password
        hashed_password = get_password_hash(FIRST_SUPERUSER_PASSWORD)
        db_user = User(
        username="adminautobi",
        email=FIRST_SUPERUSER,
        hashed_password=hashed_password,
        full_name="Super Admin",
        functional_role="super_admin",
        is_super_user=True
        )
    

        session.add(db_user)
        session.commit()
        session.refresh(db_user)






seed.py



import random
import string
import time
from uuid import uuid4
from datetime import datetime
from sqlmodel import Session, SQLModel, create_engine, select
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from app.db import engine
from models import (
    Workspace, DatabaseType, Connection, Datasource, Metadata, Tag,
    Domain, DomainDatasourceLink, AdminLLMModelSettings, KnowledgeBase,
    KnowledgeBaseType, ConnectionStatus
)

# If your models are in the same file, adjust accordingly.
# Setup the DB engine (adjust path as needed)

# Helper functions
def random_word(min_length=3, max_length=10):
    length = random.randint(min_length, max_length)
    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))

def random_description():
    words = [random_word() for _ in range(15)]
    return ' '.join(words)

def random_scores():
    # "aggregation_support", "data_coverage", "schema_availability"
    # scale of 100
    return {
        "aggregation_support": round(random.uniform(0, 100), 2),
        "data_coverage": round(random.uniform(0, 100), 2),
        "schema_availability": round(random.uniform(0, 100), 2)
    }

def random_connection_status():
    # from the enum: in_progress, pass, fail
    return random.choice(["in_progress", "pass", "fail"])

def random_knowledgebase_type():
    return random.choice([KnowledgeBaseType.KPIDEFINITION, KnowledgeBaseType.TARGETRESPONSE])

def random_connection_status_for_kb():
    return random.choice([ConnectionStatus.PASS, ConnectionStatus.FAIL, ConnectionStatus.INPROGRESS])

# Start inserting data
with Session(engine) as session:
    # 1. Create 2-3 active workspaces
    num_workspaces = random.randint(2, 3)
    workspaces = []
    for _ in range(num_workspaces):
        ws = Workspace(
            id=uuid4(),
            description=random_description(),
            is_public=False,
            is_active=True
        )
        workspaces.append(ws)
    session.add_all(workspaces)
    session.commit()

    # 2. Create database types: Postgres, MySQL, BigQuery, Oracle
    db_types_list = ["Postgres", "MySQL", "BigQuery", "Oracle"]
    db_types_map = {}
    for dt_name in db_types_list:
        dt = DatabaseType(
            id=uuid4(),
            name=dt_name
        )
        session.add(dt)
        db_types_map[dt_name] = dt
    session.commit()

    # 3. Create some connections for each workspace
    # Each workspace should have random connections with random db_types
    connections = []
    for ws in workspaces:
        # Let's say each workspace has between 2 to 4 connections
        num_connections = random.randint(2, 4)
        for _ in range(num_connections):
            dt = random.choice(list(db_types_map.values()))
            conn = Connection(
                id=uuid4(),
                name="conn_" + random_word(),
                db_type=dt.id,
                workspace_id=ws.id
            )
            connections.append(conn)
    session.add_all(connections)
    session.commit()

    # 4. For each connection, create 10-15 datasources
    # Each datasource is linked to a connection and workspace
    # Also create Metadata for each datasource
    datasources = []
    metadatas = []
    tags_list = ["finance", "marketing", "hr", "sales", "analytics", "production", "something", "srk" , "deepika",
                 "openai" , "google" , "gemini" , "microfost" , "eric" , "elon" , "doge", "twitter", "xmas"]

    for conn in connections:
        num_datasources = random.randint(10, 15)
        for _ in range(num_datasources):
            ds_id = uuid4()
            ds_scores = random_scores()
            # calculate db_score as average of these three
            # db_score is not a field in the table, but we will store scores in metadata
            # The search endpoints will compute db_score on the fly or we can just store these scores in metadata
            # and compute in the response. We'll just store them in Metadata.scores.

            ds = Datasource(
                id=ds_id,
                description=random_description(),
                connection_string=conn.name,
                last_updated_date=datetime.utcnow(),
                last_metadata_refresh_date=datetime.utcnow(),
                last_metadata_update_date=datetime.utcnow(),
                chunk_id=uuid4(),
                technology_type=conn.db_type,  # should point to a db_type
                connection_id=conn.id,
                workspace_id=conn.workspace_id,
                # For simplicity, let's just name equal to random_word
                # We'll ensure is_active is True (since we are testing active scenarios)
                name="ds_" + random_word(),
                is_active=True
            )
            datasources.append(ds)

            # Create metadata
            meta = Metadata(
                id=uuid4(),
                datasource_id=ds_id,
                is_connected=random_connection_status(),
                schema_definition_status=random_connection_status(),
                statistics_status=random_connection_status(),
                scores=ds_scores,
                db_score= round(random.uniform(0, 100), 2)
            )
            metadatas.append(meta)

    session.add_all(datasources)
    session.add_all(metadatas)
    session.commit()

    # Add random tags to some datasources
    # We'll randomly pick some datasources and assign random tags
    all_ds_ids = [d.id for d in datasources]
    random.shuffle(all_ds_ids)
    # Let's say half of them get tags
    tagged_ds_ids = all_ds_ids[:int(len(all_ds_ids)//1.3)]

    tags_objs = []
    for ds_id in tagged_ds_ids:
        # assign 1-3 random tags
        num_tags = random.randint(1, 5)
        chosen_tags = random.sample(tags_list, num_tags)
        for tg in chosen_tags:
            t = Tag(
                id=uuid4(),
                object_type="datasource",
                object_id=ds_id,
                tag_name=tg,
            )
            tags_objs.append(t)

    session.add_all(tags_objs)
    session.commit()

print("Database has been populated successfully!")

print("Populating with Domains, llm models, Knowledge Bases, Tags and linking datasaources with Domains..")
time.sleep(2)

# Assuming engine already created and tables are up from previous script

with Session(engine) as session:
    # Retrieve previously created workspaces, connections, datasources, db_types
    workspaces = session.exec(select(Workspace)).all()
    db_types = session.exec(select(DatabaseType)).all()
    connections = session.exec(select(Connection)).all()
    datasources = session.exec(select(Datasource)).all()

    # Create a few AdminLLMModelSettings
    llm_models = []
    for _ in range(3):
        llm = AdminLLMModelSettings(
            id=uuid4(),
            llm_model_name="llm_" + random_word(),
            provider_id=uuid4(),
            llm_base_endpoint_url="https://example-llm.com/api",
            llm_api_token="secret_token_" + random_word(),
            is_configured=True
        )
        llm_models.append(llm)
    session.add_all(llm_models)
    session.commit()

    # We'll re-use the same tag_list as for datasources
    tags_list = ["finance", "marketing", "hr", "sales", "analytics", "production", "elon" , "musk" , "doge",
                 "twitter" , "autobi" , "cherla" , "incedo"]

    # Create domains
    # For each workspace, create some domains (2-5)
    domains = []
    knowledgebases = []
    domain_tag_objs = []
    domain_datasource_links = []
    for ws in workspaces:
        # Filter connections that belong to this workspace
        ws_connections = [c for c in connections if c.workspace_id == ws.id]
        if not ws_connections:
            # If no connections for this workspace, skip domain creation or make connection_id None
            continue

        num_domains = random.randint(2, 5)
        for _ in range(num_domains):
            # Pick a random connection for this domain
            conn = random.choice(ws_connections)
            # Pick a random llm model
            llm_model = random.choice(llm_models)

            dom_id = uuid4()
            dom = Domain(
                id=dom_id,
                name="dom_" + random_word(),
                description=random_description(),
                purpose=random_description(),
                is_active=True,
                workspace_id=ws.id,
                connection_id=conn.id,
                default_llm_model=llm_model.id,
                knowledgebase_validation_status=random_connection_status(),
                last_updated_date=datetime.utcnow(),
                last_metadata_refresh_date=datetime.utcnow(),
                last_metadata_update_date=datetime.utcnow()
            )
            domains.append(dom)

            # Add a KnowledgeBase entry or multiple entries for this domain
            num_kb = random.randint(1, 3)
            for _kb in range(num_kb):
                kb = KnowledgeBase(
                    id=uuid4(),
                    domain_id=dom_id,
                    knowledgebase_type=random_knowledgebase_type(),
                    value={"sample_key": "sample_value"},
                    status=random_connection_status_for_kb(),
                    knowledgebase_chunk_id=uuid4()
                )
                knowledgebases.append(kb)

            # Assign tags to domain
            num_tags = random.randint(3, 7)
            chosen_tags = random.sample(tags_list, num_tags)
            for tg in chosen_tags:
                t = Tag(
                    id=uuid4(),
                    object_type="domain",
                    object_id=dom_id,
                    tag_name=tg
                )
                domain_tag_objs.append(t)

            # Associate datasources with domain
            # Filter datasources that belong to the same workspace
            ws_datasources = [ds for ds in datasources if ds.workspace_id == ws.id and ds.is_active]
            # Pick random 2-4 datasources
            if ws_datasources:
                link_count = random.randint(2, min(4, len(ws_datasources)))
                linked_ds = random.sample(ws_datasources, link_count)
                for dds in linked_ds:
                    dlink = DomainDatasourceLink(
                        domain_id=dom_id,
                        datasource_id=dds.id
                    )
                    domain_datasource_links.append(dlink)

    session.add_all(domains)
    session.add_all(knowledgebases)
    session.add_all(domain_tag_objs)
    session.add_all(domain_datasource_links)
    session.commit()

print("Database seeded with domains, tags, knowledgebases, and domain-datasource links!")



    
